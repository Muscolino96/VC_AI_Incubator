# Testing Patterns

**Analysis Date:** 2026-02-28

## Test Framework

**Runner:**
- pytest 8.0.0+ (`pytest>=8.0.0` in requirements.txt)
- Config: No dedicated pytest.ini or pyproject.toml config file found (uses pytest defaults)

**Assertion Library:**
- pytest built-in assertions and `pytest.raises()` context manager
- `jsonschema.validate()` for schema validation assertions

**Run Commands:**
```bash
pytest tests/ -v                    # Run all tests with verbose output
pytest tests/test_<module>.py -v   # Run specific test module
pytest tests/ -k "<pattern>" -v    # Run tests matching pattern
```

## Test File Organization

**Location:**
- Tests co-located in `tests/` directory at project root
- Structure mirrors source: if `vc_agents/providers/base.py` exists, tests are `tests/test_providers.py`
- Configuration and fixtures in `tests/conftest.py`

**Naming:**
- Test files: `test_<module>.py` (e.g., `test_providers.py`, `test_json_extraction.py`, `test_pipeline.py`)
- Test classes: `Test<Feature>` (e.g., `TestProviderApiKeyOverride`, `TestExtractJson`, `TestPipelineMock`)
- Test methods: `test_<behavior>` (e.g., `test_plain_json`, `test_missing_field_fails`, `test_resume_skips_completed_stages`)

**Directory Structure:**
```
tests/
├── __init__.py
├── conftest.py                 # Shared fixtures
├── test_json_extraction.py      # JSON utility tests
├── test_pipeline.py             # End-to-end pipeline tests
├── test_providers.py            # Provider API key override tests
└── test_schemas.py              # Schema validation tests
```

## Test Structure

**Suite Organization:**
```python
class TestExtractJson:
    def test_plain_json(self):
        raw = '{"key": "value"}'
        assert extract_json(raw) == '{"key": "value"}'

    def test_markdown_fenced_json(self):
        raw = '```json\n{"key": "value"}\n```'
        assert extract_json(raw) == '{"key": "value"}'
```

**Patterns:**
- Test classes group related test cases by feature/component
- One test method per specific behavior
- Arrange-Act-Assert pattern: setup input → call function → verify output
- Fixture injection via pytest fixtures (defined in conftest.py): `def test_method(self, mock)`, `def test_method(self, providers)`

## Fixtures and Factories

**Test Data:**
```python
@pytest.fixture
def mock() -> MockProvider:
    """Single mock provider for unit tests."""
    return MockProvider("test-provider")

@pytest.fixture
def providers() -> list[MockProvider]:
    """Four mock providers matching the pipeline's default provider names."""
    return [
        MockProvider("openai"),
        MockProvider("anthropic"),
        MockProvider("deepseek"),
        MockProvider("gemini"),
    ]

@pytest.fixture
def run_dir(tmp_path: Path) -> Path:
    """Temporary directory for pipeline output files."""
    out = tmp_path / "out" / "test_run"
    out.mkdir(parents=True)
    return out
```

**Location:**
- Fixtures defined in `tests/conftest.py`
- Mock data generated by `MockProvider` class (in `vc_agents/providers/mock.py`)
- Temporary directories created via pytest's `tmp_path` fixture

**Test Data Pattern:**
- MockProvider detects stage from prompt keywords and returns valid JSON matching schemas
- Each test uses descriptive setup (e.g., `monkeypatch.chdir(tmp_path)` to isolate file I/O)

## Mocking

**Framework:** pytest built-in fixtures and monkeypatch

**Patterns:**
```python
def test_openai_uses_override(self, monkeypatch):
    monkeypatch.delenv("OPENAI_API_KEY", raising=False)
    provider = OpenAIResponses(api_key="test-key-123")
    assert provider.config.require_api_key() == "test-key-123"

def test_missing_key_raises(self, monkeypatch):
    monkeypatch.delenv("OPENAI_API_KEY", raising=False)
    provider = OpenAIResponses()
    with pytest.raises(ProviderError, match="Missing API key"):
        provider.config.require_api_key()
```

**What to Mock:**
- Environment variables: `monkeypatch.delenv()`, `monkeypatch.setenv()`
- Working directory: `monkeypatch.chdir(tmp_path)`
- Provider instances: use MockProvider instead of real providers for unit/integration tests

**What NOT to Mock:**
- Provider constructors and public methods (test them directly)
- Schema validation logic (test real validation behavior)
- File system operations in pipeline tests (use `tmp_path` fixture for isolation)

## Test Types

**Unit Tests:**
- Scope: Individual functions and classes
- Examples: `test_json_extraction.py` (tests `extract_json()` utility with various inputs)
- Approach: Direct function calls with test data, immediate assertion

**Integration Tests:**
- Scope: Multiple components working together
- Examples: `test_schemas.py` (tests MockProvider generates valid JSON for each schema)
- Approach: Create provider instance, call generate, validate output against schema

**E2E Tests:**
- Scope: Full pipeline end-to-end
- Examples: `test_pipeline.py::TestPipelineMock::test_full_pipeline_produces_expected_files`
- Framework: pytest (not separate E2E framework)
- Approach: `run_pipeline(use_mock=True, ...)` with mock providers, verify all stage output files exist and contain expected data

## Common Patterns

**Async Testing:**
Not applicable; codebase uses synchronous HTTP (httpx.Client) and ThreadPoolExecutor for concurrency.

**Error Testing:**
```python
def test_missing_field_fails(self):
    bad_idea = {
        "idea_id": "x",
        "title": "X",
        # missing summary, target_customer, etc.
    }
    with pytest.raises(ValidationError):
        validate(instance=bad_idea, schema=IDEA_CARD_SCHEMA)

def test_score_range(self):
    bad = {
        "idea_id": "x",
        "reviewer_provider": "test",
        "score": 11,  # out of range
        "top_strength": "s",
        "top_weakness": "w",
        "suggestion": "s",
    }
    with pytest.raises(ValidationError):
        validate(instance=bad, schema=FEEDBACK_SCHEMA)
```

**File I/O Testing:**
```python
def test_full_pipeline_produces_expected_files(self, tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)

    run_dir = run_pipeline(
        use_mock=True,
        concurrency=1,
        retry_max=2,
        max_iterations=2,
        ideas_per_provider=5,
    )

    assert run_dir.exists()
    assert (run_dir / "stage1_ideas.jsonl").exists()
    assert (run_dir / "stage1_feedback.jsonl").exists()
```

**Count/Content Validation:**
```python
def test_ideas_count(self, tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)

    run_dir = run_pipeline(
        use_mock=True, concurrency=1, retry_max=1,
        max_iterations=1, ideas_per_provider=5,
    )

    ideas = _read_jsonl(run_dir / "stage1_ideas.jsonl")
    assert len(ideas) == 20  # 4 providers x 5 ideas
```

## Coverage

**Requirements:** Not enforced (no coverage configuration found)

**Current State:**
- Test suite covers core functionality: JSON extraction, provider API key handling, schema validation, full pipeline
- Tests use MockProvider to avoid external API dependencies
- File output verified in pipeline tests

## Test Utilities

**Helper Functions:**
- `_read_jsonl(path: Path) -> list[dict]`: Read JSONL file and parse JSON objects (defined in `test_pipeline.py`)
- `_find_json_field(prompt: str, field: str, fallback: str) -> str`: Extract JSON field from prompt (used by MockProvider)
- `json.loads()`: Standard library parsing for JSON validation

---

*Testing analysis: 2026-02-28*
