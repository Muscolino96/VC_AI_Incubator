# External Integrations

**Analysis Date:** 2026-02-28

## APIs & External Services

**LLM Providers (4 slots, configurable via pipeline.yaml):**

**Slot 1 - OpenAI Responses API:**
- Service: OpenAI GPT models via Responses API (fixed)
- SDK/Client: httpx 0.27.0+
- Auth: Bearer token in `Authorization` header
- Env var: `OPENAI_API_KEY`
- Endpoint: `https://api.openai.com/v1/responses`
- Implementation: `vc_agents/providers/openai_responses.py`
- Request format: `{ model, input: [{ role, content }], text: { format: { type: "json_object" } } }`
- Response parsing: Extracts `output[].content[].text` from response payload

**Slot 2 - Anthropic Messages API:**
- Service: Anthropic Claude models via Messages API (fixed)
- SDK/Client: httpx 0.27.0+
- Auth: API key in `x-api-key` header
- Env var: `ANTHROPIC_API_KEY`
- Endpoint: `https://api.anthropic.com/v1/messages`
- Implementation: `vc_agents/providers/anthropic_messages.py`
- Request format: `{ model, max_tokens, messages: [{ role, content }], system? }`
- max_tokens lookup: Model-aware ceiling (Claude Opus 4.x → 32K, Claude Sonnet → 64K, Haiku 4 → 16K, default 16K)
- Response parsing: Extracts `content[0].text` from response

**Slot 3 & 4 - OpenAI-Compatible Chat (DeepSeek, Gemini, others):**
- Service: OpenAI-compatible chat completion API (flexible, supports multiple providers)
- SDK/Client: httpx 0.27.0+
- Auth: Bearer token in `Authorization` header
- Env vars: `OPENAI_COMPAT_API_KEY` (shared), provider-specific overrides optional
- Base URLs (configurable in pipeline.yaml):
  - Slot 3 (DeepSeek default): `https://api.deepseek.com/v1` → env override `DEEPSEEK_BASE_URL`
  - Slot 4 (Gemini default): `https://generativelanguage.googleapis.com/v1beta/openai` → env override `GEMINI_BASE_URL`
- Implementation: `vc_agents/providers/openai_compatible_chat.py`
- Request format: `{ model, messages: [{ role, content }], response_format: { type: "json_object" } }`
- Response parsing: Extracts `choices[0].message.content`

**Retry & Timeout Behavior:**
- All providers use httpx with uniform retry config: max 3 attempts, exponential backoff (1s base, 30s max)
- Retryable status codes: 429 (rate limit), 500, 502, 503, 504 (transient server errors)
- Timeouts: 10s connect, 600s read, 10s write, 10s pool (base.py `_default_timeout()`)
- Token usage tracking: All providers record `input_tokens` + `output_tokens` per call

## Data Storage

**Databases:**
- None. The pipeline is stateless between runs.

**File Storage:**
- Local filesystem only
- Output directory: `out/run_<id>/` contains:
  - `checkpoint.json` - Resumable pipeline state (stages completed, file references)
  - `stage1_ideas_v{n}.jsonl` - Ideas generated by each founder
  - `stage1_feedback_{founder}.jsonl` - Cross-feedback on ideas
  - `stage2_{founder}_plan_v{n}.jsonl` - Startup plans per founder per iteration
  - `stage2_{founder}_reviews_round{n}.jsonl` - Advisor feedback per round
  - `stage3_{founder}_pitch.jsonl` - Final pitches
  - `stage3_investor_decisions.jsonl` - Investment decisions and portfolio ranking
  - `run_report.txt` - Human-readable final report

**Caching:**
- No persistent caching layer
- In-memory model weights via API provider SDKs (httpx client reuse)
- Mock provider: `vc_agents/providers/mock.py` used in `--use-mock` mode (no external calls)

## Authentication & Identity

**Auth Provider:**
- None. Custom API key management.
- Auth strategy: Environment variable load at runtime via `python-dotenv`
- Key validation: `vc_agents/pipeline/validate_keys.py` offers presence-only or live API checks

**API Key Management:**
- Keys stored in `.env` file (local development) or environment (production)
- Validation tool: `python -m vc_agents.pipeline.validate_keys [--live]`
- Presence check: Verifies all required env vars are set
- Live check: Makes minimal HTTP request to each provider (e.g., 16-token ping)
- No key rotation or secret manager integration (external services manage keys)

## Monitoring & Observability

**Error Tracking:**
- None (no external service). Logging to console and optional file via `vc_agents/logging_config.py`

**Logs:**
- Structured logging via Python logging module
- Log levels: DEBUG (verbose), INFO (events), WARN (retries), ERROR (failures)
- Logged events:
  - API calls: provider name, attempt number, latency (ms), success/failure
  - Retries: HTTP status, attempt count, backoff time
  - Pipeline events: stage transitions, founder decisions, advisor feedback
  - Schema validation failures with detail
- No external aggregation (logs to stderr by default, can be redirected)

## CI/CD & Deployment

**Hosting:**
- No built-in deployment. Runs as CLI tool or FastAPI server.
- Server deployment: `python -m vc_agents.web.server` (Uvicorn on port 8000)
- CLI deployment: `python -m vc_agents.pipeline.run [options]` (background process or cron job)

**CI Pipeline:**
- None (external to this codebase)
- Suggested integration: Run `pytest tests/ -v` on commit

## Environment Configuration

**Required env vars (for live pipeline):**
```
OPENAI_API_KEY=sk-...                    # Required for Slot 1 (OpenAI)
ANTHROPIC_API_KEY=sk-ant-...             # Required for Slot 2 (Claude)
OPENAI_COMPAT_API_KEY=...                # Required for Slots 3 & 4 (DeepSeek, Gemini)
GEMINI_API_KEY=...                       # Optional if using separate Gemini key
```

**Optional env vars (base URL overrides):**
```
OPENAI_COMPAT_BASE_URL=https://...       # Custom OpenAI-compatible endpoint
DEEPSEEK_BASE_URL=https://...            # Custom DeepSeek endpoint
GEMINI_BASE_URL=https://...              # Custom Gemini endpoint
```

**Optional env vars (pipeline behavior):**
```
USE_MOCK=0|1                             # 1 = use mock providers (no API calls)
CONCURRENCY=1                            # Parallel API calls (default 1)
RETRY_MAX=3                              # Max retry attempts (default 3)
MAX_ITERATIONS=3                         # Stage 2 feedback rounds (default 3)
IDEAS_PER_PROVIDER=5                     # Ideas per founder in Stage 1 (default 5)
```

**Secrets location:**
- `.env` file in project root (listed in `.gitignore`, never committed)
- Template: `.env.example` (safe to commit, contains placeholder keys)

## Webhooks & Callbacks

**Incoming:**
- None. The pipeline is request-response only.

**Outgoing:**
- None. No external notifications or callbacks.

**WebSocket (Internal):**
- Dashboard uses WebSocket for live progress streaming
- Connection: `ws://localhost:8000/ws/{run_id}`
- Messages: PipelineEvent objects (JSON) broadcast to all connected clients
- Implemented in `vc_agents/web/server.py` (`@app.websocket("/ws/{run_id}")`

## Rate Limiting & Quotas

**Provider-specific:**
- OpenAI: Rate limits via API (429 responses trigger automatic retry)
- Anthropic: Rate limits via API (429 responses trigger automatic retry)
- DeepSeek: Rate limits via API, default base URL has no explicit per-user quota
- Gemini: Rate limits via API; free tier has 15 RPM cap (documented in models_catalog.yaml)

**Application-level:**
- Concurrency limiter: `concurrency` setting in pipeline.yaml (default 1, max tested is 1)
- Exponential backoff on 429: min 1s, max 30s, doubles each retry

## Cost Tracking

**Pricing Reference:**
- `models_catalog.yaml` contains pricing for 20+ models across 4 tiers
- Pricing format: `{ input: $/1M tokens, output: $/1M tokens }`
- Cost estimator: `vc_agents/pipeline/cost_estimator.py` calculates run cost
- Typical run cost: $8-14 using frontier models (GPT-5.2 + Claude Opus 4.5)
- Budget preset available: $1-3 using efficient tier models (Claude Haiku, GPT-5-mini)

## API Reference Integration

**No generated API documentation.** Providers are configured declaratively:
- Provider list in `pipeline.yaml` under `providers:` key
- Each provider specifies: name, type (openai_responses|anthropic_messages|openai_compatible_chat), model, api_key_env, optional base_url
- Model catalog in `models_catalog.yaml` lists all supported models with metadata (context window, strengths, weaknesses, pricing)

---

*Integration audit: 2026-02-28*
