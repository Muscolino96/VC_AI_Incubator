---
phase: 02-pre-flight-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - vc_agents/pipeline/run.py
  - tests/test_pipeline.py
autonomous: true
requirements: [PRE-01, PRE-02, PRE-03, PRE-04, PRE-05]

must_haves:
  truths:
    - "Starting the pipeline fires a minimal probe to every configured provider before Stage 1 begins"
    - "Each probe uses the exact model string from pipeline.yaml, not a generic fallback"
    - "All four probes run in parallel; the pre-flight step finishes in under 10 seconds on success"
    - "When a provider fails, the error message names the provider and explains why (HTTP status, bad key, wrong model, unreachable URL)"
    - "Passing --skip-preflight bypasses pre-flight entirely with no errors"
    - "pytest tests/ -v passes after the change"
  artifacts:
    - path: "vc_agents/pipeline/run.py"
      provides: "run_preflight() function + --skip-preflight CLI wiring"
      contains: "def run_preflight"
    - path: "tests/test_pipeline.py"
      provides: "TestPreflight class with tests covering pass, failure, and skip cases"
      contains: "class TestPreflight"
  key_links:
    - from: "run_pipeline()"
      to: "run_preflight()"
      via: "called before Stage 1, skipped when skip_preflight=True"
      pattern: "run_preflight\\(providers"
    - from: "run_preflight()"
      to: "_map_concurrently"
      via: "parallel probe execution"
      pattern: "_map_concurrently\\(.*preflight"
    - from: "parse_args()"
      to: "run_pipeline()"
      via: "skip_preflight=args.skip_preflight"
      pattern: "skip_preflight"
---

<objective>
Add mandatory pre-flight validation to the pipeline: before Stage 1 begins, probe every
configured provider with a 1-token call using the exact configured model. Run all probes
in parallel. Fail fast with a clear error if any provider is misconfigured. Let the user
bypass with --skip-preflight.

Purpose: Users know immediately if any provider is broken before the pipeline spends
tokens doing real work.
Output: run_preflight() function in run.py; --skip-preflight CLI flag; TestPreflight
tests covering all 5 requirements.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/01-parallelization/01-03-SUMMARY.md
</context>

<interfaces>
<!-- Key contracts the executor needs. Extracted from vc_agents/pipeline/run.py. -->

From vc_agents/pipeline/run.py — _map_concurrently:
```python
def _map_concurrently(func: Any, items: list[Any], concurrency: int) -> Iterable[Any]:
    if concurrency <= 1:
        for item in items:
            yield func(item)
        return
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        for result in executor.map(func, items):
            yield result
```

From vc_agents/pipeline/run.py — run_pipeline() signature:
```python
def run_pipeline(
    use_mock: bool,
    concurrency: int,
    retry_max: int,
    max_iterations: int = 3,
    ideas_per_provider: int = 5,
    sector_focus: str = "",
    emit: EventCallback = noop_callback,
    provider_config: dict[str, Any] | None = None,
    resume_dir: Path | None = None,
    roles_config: dict[str, Any] | None = None,
    deliberation_enabled: bool = False,
    slot3_base_url: str = "https://api.deepseek.com/v1",
    slot4_base_url: str = "https://generativelanguage.googleapis.com/v1beta/openai",
) -> Path:
```

From vc_agents/pipeline/run.py — run_pipeline() entry point (pre-Stage 1):
```python
# After roles are assigned and run_dir is set up, before Stage 1:
try:
    # Stage 1: Ideate and Select
```

From vc_agents/pipeline/run.py — parse_args():
```python
def parse_args(argv: list[str]) -> argparse.Namespace:
    # ... existing args ...
    parser.add_argument("--deliberation", action="store_true", ...)
    parser.add_argument("--estimate-cost", action="store_true", ...)
    return parser.parse_args(argv)
```

From vc_agents/pipeline/run.py — main() call to run_pipeline:
```python
run_dir = run_pipeline(
    use_mock=use_mock,
    concurrency=args.concurrency,
    retry_max=args.retry_max,
    max_iterations=args.max_iterations,
    ideas_per_provider=args.ideas_per_provider,
    sector_focus=args.sector_focus,
    resume_dir=resume_dir,
    roles_config=roles_override,
    deliberation_enabled=args.deliberation,
)
```

From vc_agents/providers/base.py — BaseProvider:
```python
class BaseProvider(abc.ABC):
    name: str
    model: str
    # Subclasses: OpenAIResponses, AnthropicMessages, OpenAICompatibleChat, MockProvider
    def generate(self, prompt: str, system: str = "", ...) -> str: ...
```

From vc_agents/providers/base.py — ProviderError:
```python
class ProviderError(RuntimeError):
    """Raised when a provider call fails."""
```

From vc_agents/providers/mock.py — MockProvider.generate:
```python
# MockProvider.generate() returns valid JSON strings without making network calls.
# Pre-flight logic must skip real HTTP probes and treat MockProvider as always passing.
```

From pipeline.yaml — provider config structure (for understanding what model/api_key fields look like):
```yaml
providers:
  - name: openai
    type: openai_responses
    model: gpt-5.2
    api_key_env: OPENAI_API_KEY
  - name: anthropic
    type: anthropic_messages
    model: claude-opus-4-5
    api_key_env: ANTHROPIC_API_KEY
  - name: deepseek
    type: openai_compatible_chat
    model: deepseek-reasoner
    api_key_env: OPENAI_COMPAT_API_KEY
    base_url: https://api.deepseek.com/v1
  - name: gemini
    type: openai_compatible_chat
    model: gemini-3.1-pro-preview
    api_key_env: GEMINI_API_KEY
    base_url: https://generativelanguage.googleapis.com/v1beta/openai
```
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Implement run_preflight() and wire into run_pipeline()</name>
  <files>vc_agents/pipeline/run.py</files>
  <behavior>
    - run_preflight(providers, concurrency) calls provider.generate("Reply: ok", system="", max_tokens=4) for each provider, in parallel via _map_concurrently
    - On success for all providers: logs "Pre-flight OK" and returns normally
    - On failure for any provider: raises PreflightError("Pre-flight failed:\n  - openai: HTTP 401 — invalid API key\n  - gemini: HTTP 404 — model 'bad-model' not found")
    - PreflightError message lists ONLY the failed providers with their specific failure reason
    - MockProvider instances are detected by isinstance check and treated as always passing (no real HTTP call, return a dummy CheckResult with ok=True)
    - run_pipeline() gains a skip_preflight: bool = False parameter
    - When skip_preflight=False and use_mock=False: run_preflight(providers, concurrency) is called after providers are built but before roles assignment or Stage 1
    - When skip_preflight=True: pre-flight is bypassed entirely (no log noise, no error)
    - When use_mock=True: pre-flight is bypassed automatically (MockProvider is already safe)
    - parse_args() gains --skip-preflight flag (store_true)
    - main() passes skip_preflight=args.skip_preflight to run_pipeline()
  </behavior>
  <action>
Add to vc_agents/pipeline/run.py:

1. Define PreflightError(RuntimeError) class near the top (after ProviderError import).

2. Define a PreflightResult dataclass: name: str, ok: bool, detail: str

3. Define run_preflight(providers: list[BaseProvider], concurrency: int) -> None:
   - Inner function probe(provider: BaseProvider) -> PreflightResult:
     - If isinstance(provider, MockProvider): return PreflightResult(provider.name, True, "mock — skipped")
     - Call provider.generate("Reply: ok", system="", max_tokens=4) wrapped in try/except Exception as e
     - On success: return PreflightResult(provider.name, True, f"OK (model={provider.model})")
     - On exception: return PreflightResult(provider.name, False, str(e)[:300])
   - Run: results = list(_map_concurrently(probe, providers, concurrency))
   - failures = [r for r in results if not r.ok]
   - If failures: raise PreflightError("Pre-flight failed:\n" + "\n".join(f"  - {r.name}: {r.detail}" for r in failures))
   - Else: logger.info("Pre-flight OK: all %d providers reachable", len(results))

4. Add skip_preflight: bool = False parameter to run_pipeline() signature (after deliberation_enabled).

5. In run_pipeline(), after providers list is built (both YAML and fallback paths) but before the roles/RoleAssignment block: insert the guard:
   ```python
   if not skip_preflight and not use_mock:
       run_preflight(providers, concurrency)
   ```

6. In parse_args(), add:
   ```python
   parser.add_argument(
       "--skip-preflight", action="store_true",
       help="Skip pre-flight provider validation (use when keys are known good)",
   )
   ```

7. In main(), pass skip_preflight=args.skip_preflight to run_pipeline().

IMPORTANT: use_mock=True already means MockProvider is used and run_preflight short-circuits the MockProvider check inside probe() anyway — but the outer guard `not use_mock` means we never even call run_preflight in mock mode, keeping mock runs fast and unconditional.

IMPORTANT: max_tokens=4 is a sentinel value intentionally small. The generate() call through the real provider stack will issue a live API call. If the provider is unreachable or the key/model is wrong, generate() will raise ProviderError (or similar) which probe() catches as Exception.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -c "from vc_agents.pipeline.run import run_preflight, PreflightError; print('imports OK')"</automated>
  </verify>
  <done>
    - run_preflight() defined and importable
    - PreflightError defined
    - run_pipeline() has skip_preflight parameter
    - parse_args() has --skip-preflight flag
    - main() passes skip_preflight to run_pipeline()
    - All existing tests still pass (pytest tests/ -v)
  </done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Write TestPreflight tests covering all PRE requirements</name>
  <files>tests/test_pipeline.py</files>
  <behavior>
    - test_preflight_passes_with_mock_providers: run_pipeline(use_mock=True, skip_preflight=False, ...) completes without raising PreflightError — mock providers are always treated as passing
    - test_preflight_skip_preflight_flag: run_pipeline(use_mock=False, skip_preflight=True, ...) with no env vars set still completes without raising about providers (skip bypasses)
    - test_preflight_detects_failing_provider: run_preflight() with a provider whose generate() raises ProviderError raises PreflightError; the error message contains the provider name and the failure detail
    - test_preflight_lists_multiple_failures: run_preflight() with two failing providers raises PreflightError whose message contains both provider names
    - test_preflight_uses_configured_model: the PreflightResult detail from a passing MockProvider-equivalent stub contains the provider's model string (verify PRE-02 — exact model used)
    - test_preflight_runs_in_parallel: run_preflight() with concurrency=4 on 4 MockProviders-with-sleep completes faster than sequential (ratio check, same pattern as PARA-06)
  </behavior>
  <action>
Add a TestPreflight class to tests/test_pipeline.py.

Import at top of file (if not already present): from vc_agents.pipeline.run import run_preflight, PreflightError
Import: from vc_agents.providers.mock import MockProvider

For test_preflight_detects_failing_provider and test_preflight_lists_multiple_failures:
  - Create a simple stub class FailingProvider that inherits BaseProvider with name/model attributes and a generate() that raises ProviderError("HTTP 401 — invalid API key"). Do NOT use MockProvider for this — MockProvider's generate() always succeeds.
  - Alternatively monkeypatch MockProvider.generate to raise ProviderError for specific providers.

For test_preflight_uses_configured_model:
  - Confirm run_preflight() does not raise when all providers are MockProvider instances (mock bypass path). Then directly inspect that the mock bypass returns a result with ok=True rather than checking the model string in detail (since MockProvider bypass returns "mock — skipped"). The test should assert no PreflightError is raised and that run_preflight returns normally.

For test_preflight_runs_in_parallel:
  - Create 4 MockProviders. Monkeypatch MockProvider.generate to sleep 0.05s.
  - Time run_preflight(providers, concurrency=4) and run_preflight(providers, concurrency=1).
  - Assert parallel_time / sequential_time < 0.6 (generous threshold, same pattern as PARA-06).

For test_preflight_skip_preflight_flag:
  - Call run_pipeline(use_mock=False, skip_preflight=True, concurrency=1, retry_max=1, max_iterations=1, ideas_per_provider=5) in a monkeypatched environment where all 4 providers would fail if pre-flight ran. The test should NOT reach Stage 1 (it will fail when building providers without API keys). So instead, directly test that run_preflight is NOT called when skip_preflight=True by monkeypatching run_preflight to raise an AssertionError and asserting that run_pipeline(use_mock=True, skip_preflight=False, ...) completes normally (mock path skips preflight via `not use_mock` guard).

  Simpler approach for PRE-05: test that calling run_pipeline with use_mock=True, skip_preflight=False does not raise (mock providers always pass). Then separately, write a unit test that directly calls run_preflight() with mock providers and asserts it returns None (no error raised). The `--skip-preflight` CLI flag is covered by inspecting that parse_args(['--skip-preflight']).skip_preflight is True.

All tests must be in class TestPreflight. Each test takes tmp_path and monkeypatch fixtures where needed.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py -k TestPreflight -v</automated>
  </verify>
  <done>
    - TestPreflight class exists with at least 5 test methods
    - All TestPreflight tests pass
    - Full test suite still passes: pytest tests/ -v (all existing 51 + new preflight tests)
    - PRE-01 through PRE-05 all have at least one test covering the behavior
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Import check: `python -c "from vc_agents.pipeline.run import run_preflight, PreflightError; print('OK')`
2. CLI help check: `python -m vc_agents.pipeline.run --help` shows `--skip-preflight` flag
3. Full test suite: `pytest tests/ -v` — all tests pass (51 existing + new preflight tests)
4. Manual inspect: In run.py, confirm `run_preflight(providers, concurrency)` call appears inside `run_pipeline()` guarded by `if not skip_preflight and not use_mock:`
</verification>

<success_criteria>
- run_preflight() function exists in run.py, uses _map_concurrently for parallel probes
- Each probe uses provider.generate() with the provider's own .model attribute (exact configured model, not a fallback)
- MockProvider instances are bypassed inside probe() via isinstance check
- PreflightError raised on any failure; message lists each failing provider name and detail
- skip_preflight=False parameter on run_pipeline(); --skip-preflight flag in parse_args()
- When skip_preflight=True, no pre-flight call is made
- pytest tests/ -v passes (all tests including TestPreflight)
</success_criteria>

<output>
After completion, create `.planning/phases/02-pre-flight-validation/02-01-SUMMARY.md`
following the summary template at @C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</output>
