---
phase: 01-parallelization
plan: 03
type: execute
wave: 3
depends_on:
  - 01-02
files_modified:
  - vc_agents/pipeline/run.py
  - tests/test_pipeline.py
autonomous: true
requirements:
  - PARA-03
  - PARA-06
must_haves:
  truths:
    - "Stage 3 investor evaluations for a given pitch all fire concurrently"
    - "Wall-clock time with concurrency=4 is 40% or less of wall-clock time with concurrency=1 on a mock run"
    - "pytest tests/ -v passes"
  artifacts:
    - path: "vc_agents/pipeline/run.py"
      provides: "Parallelized Stage 3 investor evaluation loop"
      contains: "investor_eval_task"
    - path: "tests/test_pipeline.py"
      provides: "Wall-clock speedup test verifying PARA-06"
  key_links:
    - from: "run_stage3 investor evaluation inner loop"
      to: "_map_concurrently"
      via: "investor_eval_task extracted and mapped over investors list"
      pattern: "_map_concurrently.*investor_eval_task"
    - from: "test_para06_wall_clock_speedup"
      to: "run_pipeline(concurrency=1) and run_pipeline(concurrency=4)"
      via: "time.monotonic() measurement on mock run"
      pattern: "time.monotonic"
---

<objective>
Parallelize Stage 3 investor evaluations in run.py so all investors evaluating a given pitch fire concurrently. Add a wall-clock timing test that measures the speedup from concurrency=4 vs concurrency=1 with the mock provider, verifying the 40% threshold.

Purpose: Stage 3 has a sequential inner loop (investors evaluating each pitch). With 4 founders × 3 investors, that's 12 sequential investor calls that can all run concurrently. The PARA-06 timing test provides measurable proof that the parallelization work across all three plans is achieving the wall-clock reduction target.

Output: run.py with parallelized Stage 3 investor calls; timing test proving ≤40% wall-clock ratio.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@vc_agents/pipeline/run.py
@tests/test_pipeline.py
@tests/conftest.py
@.planning/phases/01-parallelization/01-01-SUMMARY.md
@.planning/phases/01-parallelization/01-02-SUMMARY.md
</context>

<interfaces>
<!-- Key interfaces from run.py after Plans 01 and 02 are applied. -->

From vc_agents/pipeline/run.py (Stage 3 current structure):

```python
def run_stage3(
    providers: list[BaseProvider],
    final_plans: dict[str, dict[str, Any]],
    retry_max: int,
    concurrency: int,
    run_dir: Path,
    emit: EventCallback = noop_callback,
    roles: RoleAssignment | None = None,
) -> list[dict[str, Any]]:
    ...
    founders_list = roles.founders if roles is not None else providers
    investors_pool = roles.investors if roles is not None else providers

    all_pitches: list[dict[str, Any]] = []
    all_decisions: list[dict[str, Any]] = []

    # OUTER LOOP — one founder at a time (acceptable: pitch depends on founder's plan)
    for founder in founders_list:
        plan = next((p for p in final_plans.values() if p["founder_provider"] == founder.name), None)
        ...
        pitch = retry_json_call(founder, prompt, schema=PITCH_SCHEMA, ...)
        all_pitches.append(pitch)

        # INNER LOOP — sequential investor evaluations (PARALLELIZE THIS):
        investors = [p for p in investors_pool if p.name != founder.name]
        for investor in investors:
            decision = retry_json_call(investor, prompt, ...)
            all_decisions.append(decision)
            emit(PipelineEvent(...))
```

The outer per-founder loop (pitch creation) cannot be parallelized without restructuring because it depends on each founder's final plan being matched sequentially. However, the inner investor evaluation loop for each pitch IS independent — all investors can evaluate simultaneously.

IMPORTANT: The outer loop (pitch creation) CAN also be parallelized at module level, but it adds complexity for marginal gain (pitch creation is 1 call vs 3 investor calls). Per the spec, just parallelizing the investor evaluations is sufficient for PARA-03. Keep the outer loop sequential to avoid complexity.

Pattern for investor_eval_task:
```python
def investor_eval_task(task: dict[str, Any]) -> dict[str, Any]:
    investor = task["investor"]
    prompt = investor_prompt_tmpl.format(
        provider_name=investor.name,
        pitch_json=json.dumps(task["pitch"], indent=2),
        plan_json=json.dumps(task["plan"], indent=2),
    )
    decision = retry_json_call(
        investor, prompt, schema=INVESTOR_DECISION_SCHEMA,
        context=f"invest ({investor.name}/{task['idea_id']})", max_retries=retry_max,
        system=investor_system,
    )
    logger.info("    %s -> %s (conviction: %s)", investor.name, decision["decision"], decision["conviction_score"])
    emit(PipelineEvent(
        type=EventType.STEP_COMPLETE, stage="stage3", step="investor_decision",
        provider=investor.name, idea_id=task["idea_id"],
        message=f"{investor.name}: {decision['decision']} (conviction {decision['conviction_score']})",
        data={"decision": decision["decision"], "conviction": decision["conviction_score"]},
    ))
    return decision

investor_tasks = [
    {"investor": investor, "pitch": pitch, "plan": plan, "idea_id": idea_id}
    for investor in investors
]
decisions = list(_map_concurrently(investor_eval_task, investor_tasks, concurrency))
all_decisions.extend(decisions)
```

From vc_agents/providers/mock.py (for timing test context):
- MockProvider.generate() returns immediately with minimal latency (no sleep by default)
- With concurrency=1, _map_concurrently runs sequentially (N iterations)
- With concurrency=N, _map_concurrently runs in parallel (1 iteration wall-clock equivalent)
- The timing test needs a MockProvider with a small sleep to make the difference measurable

TIMING TEST APPROACH:
The mock provider currently has no artificial latency. To make the timing test robust:
1. Monkeypatch a sleep (e.g., 0.05s) into MockProvider.generate() for the timing test
2. Run concurrency=1 and concurrency=4, compare wall-clock times
3. With ~8 concurrent calls (4 founders × idea gen + selection + Stage 3) and 0.05s each:
   - Sequential: ~40 calls × 0.05s = ~2s
   - Concurrent: ~0.2s (overlapping)
   - Ratio: ~10% (well within the 40% threshold)

Alternative: No monkeypatching needed if we just rely on the ThreadPoolExecutor overhead difference being measurable. However, with zero-latency mock, the ratio may fluctuate. Use monkeypatching with a small sleep for determinism.
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Parallelize Stage 3 investor evaluations</name>
  <files>vc_agents/pipeline/run.py</files>
  <behavior>
    - investor_eval_task inner function defined inside run_stage3
    - Accepts dict with: investor, pitch, plan, idea_id
    - Returns investor decision dict
    - _map_concurrently(investor_eval_task, investor_tasks, concurrency) replaces sequential for-loop
    - all_decisions.extend(decisions) collects results
    - emit() calls happen inside investor_eval_task (thread-safe)
    - Aggregate JSONL writes (_write_jsonl for pitches and decisions) unchanged
    - All existing tests pass
  </behavior>
  <action>
In run_stage3(), locate the inner sequential investor loop:
```python
for investor in investors:
    prompt = investor_prompt_tmpl.format(...)
    decision = retry_json_call(...)
    all_decisions.append(decision)
    ...emit(...)
```

Replace with:

```python
def investor_eval_task(task: dict[str, Any]) -> dict[str, Any]:
    investor = task["investor"]
    prompt = investor_prompt_tmpl.format(
        provider_name=investor.name,
        pitch_json=json.dumps(task["pitch"], indent=2),
        plan_json=json.dumps(task["plan"], indent=2),
    )
    decision = retry_json_call(
        investor, prompt, schema=INVESTOR_DECISION_SCHEMA,
        context=f"invest ({investor.name}/{task['idea_id']})",
        max_retries=retry_max,
        system=investor_system,
    )
    logger.info(
        "    %s -> %s (conviction: %s)",
        investor.name, decision["decision"], decision["conviction_score"],
    )
    emit(PipelineEvent(
        type=EventType.STEP_COMPLETE, stage="stage3", step="investor_decision",
        provider=investor.name, idea_id=task["idea_id"],
        message=f"{investor.name}: {decision['decision']} (conviction {decision['conviction_score']})",
        data={"decision": decision["decision"], "conviction": decision["conviction_score"]},
    ))
    return decision

investor_tasks = [
    {"investor": investor, "pitch": pitch, "plan": plan, "idea_id": idea_id}
    for investor in investors
]
decisions = list(_map_concurrently(investor_eval_task, investor_tasks, concurrency))
all_decisions.extend(decisions)
```

The `idea_id` variable is available from `plan["idea_id"]` — add `idea_id = plan.get("idea_id", "")` before this block if not already set. Keep the outer founder loop sequential.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py::TestPipelineMock::test_investor_decisions_count tests/test_pipeline.py::TestPipelineMock::test_portfolio_report_has_all_founders -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>investor_eval_task defined inside run_stage3; _map_concurrently replaces sequential investor for-loop; investor_decisions_count test passes (still 12 decisions); portfolio_report test passes.</done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Add wall-clock speedup test (PARA-06)</name>
  <files>tests/test_pipeline.py</files>
  <behavior>
    - TestParallelization class added to test_pipeline.py
    - test_para06_wall_clock_speedup monkeypatches MockProvider.generate with a 0.05s sleep
    - Runs pipeline with concurrency=1, measures elapsed time
    - Runs pipeline with concurrency=4, measures elapsed time
    - Asserts concurrent_time <= sequential_time * 0.40
    - Test uses ideas_per_provider=2, max_iterations=1 to keep runtime short
    - Full test suite still passes
  </behavior>
  <action>
Add to tests/test_pipeline.py, after the existing test classes:

```python
import time as _time_module


class TestParallelization:
    """PARA-06: Verify wall-clock speedup from concurrency."""

    def test_para06_wall_clock_speedup(self, tmp_path, monkeypatch):
        """Concurrent run (concurrency=4) completes in ≤40% of sequential time (concurrency=1)."""
        from vc_agents.providers.mock import MockProvider

        # Patch generate to add a small artificial latency so wall-clock difference is measurable
        original_generate = MockProvider.generate

        def slow_generate(self, prompt: str, system: str = "") -> str:
            _time_module.sleep(0.05)
            return original_generate(self, prompt, system=system)

        monkeypatch.setattr(MockProvider, "generate", slow_generate)

        # Sequential baseline
        monkeypatch.chdir(tmp_path)
        t0 = _time_module.monotonic()
        run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=2,
        )
        sequential_time = _time_module.monotonic() - t0

        # Concurrent run
        t1 = _time_module.monotonic()
        run_pipeline(
            use_mock=True, concurrency=4, retry_max=1,
            max_iterations=1, ideas_per_provider=2,
        )
        concurrent_time = _time_module.monotonic() - t1

        ratio = concurrent_time / sequential_time
        print(f"\nWall-clock: sequential={sequential_time:.2f}s, concurrent={concurrent_time:.2f}s, ratio={ratio:.2%}")
        assert ratio <= 0.40, (
            f"Concurrent run took {ratio:.1%} of sequential time — expected ≤40%. "
            f"sequential={sequential_time:.2f}s, concurrent={concurrent_time:.2f}s"
        )
```

The `run_pipeline` import is already at the top of test_pipeline.py. The `import time as _time_module` line goes at the top of the file with the other imports (or use `import time` if not already present — check first and use the existing import name if `time` is already imported).
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py::TestParallelization::test_para06_wall_clock_speedup -v -s 2>&1 | tail -20</automated>
  </verify>
  <done>TestParallelization class added; test passes; ratio printed; ratio <= 0.40 assertion holds; full test suite passes.</done>
</task>

</tasks>

<verification>
Run full test suite: `cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/ -v`

All tests pass, including TestParallelization::test_para06_wall_clock_speedup.

Confirm complete parallelization coverage:
- run_stage1: _map_concurrently used for idea generation, feedback (already existed), selection
- run_stage2: _map_concurrently used for outer founder loop (_run_founder_stage2) and inner advisor reviews
- run_stage3: _map_concurrently used for investor evaluations per pitch
</verification>

<success_criteria>
- Stage 3 investor evaluations: _map_concurrently(investor_eval_task, investor_tasks, concurrency) replaces sequential loop
- PARA-06: test_para06_wall_clock_speedup passes; concurrent ratio <= 0.40
- All 6 requirement IDs addressed across the three plans:
  - PARA-01: Stage 2 outer founder loop parallelized (Plan 02)
  - PARA-02: Stage 2 inner advisor reviews parallelized (Plan 02)
  - PARA-03: Stage 3 investor evaluations parallelized (Plan 03)
  - PARA-04: Stage 1 idea generation parallelized (Plan 01)
  - PARA-05: Stage 1 selection calls parallelized (Plan 01)
  - PARA-06: Wall-clock timing test passes (Plan 03)
- Full pytest suite passes: `pytest tests/ -v` returns 0
</success_criteria>

<output>
After completion, create `.planning/phases/01-parallelization/01-03-SUMMARY.md` with:
- How investor_eval_task is structured in run_stage3
- PARA-06 timing test results (actual ratio observed)
- Summary of all parallelization changes across all three plans
- Final test suite results (pass/fail counts)
</output>
