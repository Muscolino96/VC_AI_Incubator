---
phase: 01-parallelization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - vc_agents/pipeline/run.py
  - tests/test_pipeline.py
autonomous: true
requirements:
  - PARA-04
  - PARA-05
must_haves:
  truths:
    - "Stage 1 idea generation fires all founders' calls concurrently, not one-at-a-time"
    - "Stage 1 selection fires all founders' calls concurrently, not one-at-a-time"
    - "pytest tests/ -v passes with concurrency=4 on the full mock pipeline"
  artifacts:
    - path: "vc_agents/pipeline/run.py"
      provides: "Stage 1 parallelized loops"
      contains: "_map_concurrently"
    - path: "tests/test_pipeline.py"
      provides: "Test verifying concurrent Stage 1 execution ordering"
  key_links:
    - from: "run_stage1 Step 1a (idea generation loop)"
      to: "_map_concurrently"
      via: "idea_gen_task function extracted and mapped"
      pattern: "_map_concurrently.*idea_gen_task.*founders"
    - from: "run_stage1 Step 1c (selection loop)"
      to: "_map_concurrently"
      via: "selection_task function extracted and mapped"
      pattern: "_map_concurrently.*selection_task.*founders"
---

<objective>
Parallelize Stage 1 idea generation (Step 1a) and selection (Step 1c) in run.py so all founders' independent API calls fire concurrently instead of sequentially.

Purpose: Stage 1a and 1c are the two remaining sequential loops in Stage 1 — the cross-feedback step (1b) already uses _map_concurrently. Fixing these two loops eliminates the sequential bottleneck for ~8 API calls (4 idea gen + 4 selections) that have no data dependencies between providers.

Output: run.py with parallelized Stage 1a and 1c; tests covering concurrent execution with concurrency>1.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@vc_agents/pipeline/run.py
@tests/test_pipeline.py
@tests/conftest.py
</context>

<interfaces>
<!-- Key existing interfaces the executor needs. No codebase exploration needed. -->

From vc_agents/pipeline/run.py:

```python
def _map_concurrently(func: Any, items: list[Any], concurrency: int) -> Iterable[Any]:
    """Already exists. func takes a single item, returns a result. Uses ThreadPoolExecutor."""
    if concurrency <= 1:
        for item in items:
            yield func(item)
        return
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        for result in executor.map(func, items):
            yield result

def run_stage1(
    providers: list[BaseProvider],
    ideas_per_provider: int,
    retry_max: int,
    concurrency: int,
    run_dir: Path,
    sector_focus: str = "",
    emit: EventCallback = noop_callback,
    roles: RoleAssignment | None = None,
) -> dict[str, dict[str, Any]]:
    ...
    # STEP 1a — CURRENTLY SEQUENTIAL (lines 377-400):
    for provider in founders:
        prompt = idea_prompt.format(...)
        payload = retry_json_call(provider, prompt, ...)
        all_ideas[provider.name] = idea_items
        emit(PipelineEvent(...))

    # STEP 1b — ALREADY PARALLEL (reference):
    tasks = [{"idea": idea, "reviewer": reviewer} ...]
    all_feedback = list(_map_concurrently(feedback_task, tasks, concurrency))

    # STEP 1c — CURRENTLY SEQUENTIAL (lines 445-470):
    for provider in founders:
        prompt = select_prompt.format(...)
        result = retry_json_call(provider, prompt, ...)
        selections[provider.name] = result
```

The pattern to follow for Step 1a:
1. Extract a `def idea_gen_task(provider: BaseProvider) -> tuple[str, list]:` function that returns `(provider.name, idea_items)`
2. Replace the `for provider in founders:` loop with `list(_map_concurrently(idea_gen_task, founders, concurrency))`
3. Collect results from the tuples into `all_ideas` dict

The pattern to follow for Step 1c:
1. Extract a `def selection_task(provider: BaseProvider) -> tuple[str, dict]:` function that returns `(provider.name, result)`
2. Replace the `for provider in founders:` loop with `list(_map_concurrently(selection_task, founders, concurrency))`
3. Collect results from the tuples into `selections` dict

IMPORTANT: Step 1c needs `all_ideas` and `all_feedback` in its closure — they are already computed before 1c runs, so they can be captured in the closure safely.

IMPORTANT: The `emit` calls inside each task function are thread-safe (EventCallback is a callable, Python's GIL protects simple function calls).
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Parallelize Stage 1 idea generation (Step 1a)</name>
  <files>vc_agents/pipeline/run.py</files>
  <behavior>
    - idea_gen_task extracted as inner function inside run_stage1, captures provider-independent variables in closure
    - _map_concurrently(idea_gen_task, founders, concurrency) replaces the sequential for-loop
    - Results collected into all_ideas dict in the same order (executor.map preserves order)
    - emit() calls happen inside the task function (thread-safe)
    - all_ideas dict built from list of (name, items) tuples yielded by _map_concurrently
    - flat_ideas and _write_jsonl still happen after all idea gen completes
    - existing tests still pass with concurrency=1
  </behavior>
  <action>
In run_stage1(), locate Step 1a (the sequential for-loop starting at "for provider in founders:" around line 377 in the original).

Replace the sequential loop with a task function + _map_concurrently call:

```python
# --- Step 1a: Generate ideas (parallel) ---
logger.info("Step 1a: Generating ideas (%d founders x %d ideas)", len(founders), ideas_per_provider)
all_ideas: dict[str, list[dict[str, Any]]] = {}

def idea_gen_task(provider: BaseProvider) -> tuple[str, list[dict[str, Any]]]:
    prompt = idea_prompt.format(
        provider_name=provider.name,
        ideas_count=ideas_per_provider,
    ) + sector_instruction
    payload = retry_json_call(
        provider, prompt, schema=None,
        context=f"idea generation ({provider.name})", max_retries=retry_max,
        system=idea_system,
    )
    idea_items = payload.get("ideas")
    if not isinstance(idea_items, list):
        raise ValueError(f"Idea generation ({provider.name}) did not return an ideas list.")
    for item in idea_items:
        validate_schema(item, IDEA_CARD_SCHEMA, f"idea card ({provider.name})")
    logger.info("  %s generated %d ideas", provider.name, len(idea_items))
    emit(PipelineEvent(
        type=EventType.STEP_COMPLETE, stage="stage1", step="ideas",
        provider=provider.name, message=f"Generated {len(idea_items)} ideas",
        data={"ideas": idea_items},
    ))
    return provider.name, idea_items

for name, items in _map_concurrently(idea_gen_task, founders, concurrency):
    all_ideas[name] = items
```

Do not change any other part of run_stage1. The sector_instruction variable is already computed above the loop in the original code — it remains in place.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py::TestPipelineMock::test_ideas_count tests/test_pipeline.py::TestPipelineMock::test_feedback_count -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>idea_gen_task function exists inside run_stage1; _map_concurrently replaces sequential for-loop; test_ideas_count and test_feedback_count pass.</done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Parallelize Stage 1 selection calls (Step 1c)</name>
  <files>vc_agents/pipeline/run.py, tests/test_pipeline.py</files>
  <behavior>
    - selection_task extracted as inner function inside run_stage1, captures all_ideas and all_feedback in closure
    - _map_concurrently(selection_task, founders, concurrency) replaces the sequential for-loop
    - selections dict built from (name, result) tuples
    - Test: test_concurrent_stage1_selection passes with concurrency=4, verifying all 4 selections present
    - existing tests still pass
  </behavior>
  <action>
PART A — run.py:

In run_stage1(), locate Step 1c (the sequential "for provider in founders:" loop for selections, around line 445 original).

Replace with:

```python
# --- Step 1c: Each founder selects best idea (parallel) ---
logger.info("Step 1c: Founders select best idea")
selections: dict[str, dict[str, Any]] = {}

def selection_task(provider: BaseProvider) -> tuple[str, dict[str, Any]]:
    my_ideas = all_ideas[provider.name]
    my_idea_ids = {idea["idea_id"] for idea in my_ideas}
    my_feedback = [f for f in all_feedback if f["idea_id"] in my_idea_ids]
    feedback_by_idea: dict[str, list[dict[str, Any]]] = {}
    for fb in my_feedback:
        feedback_by_idea.setdefault(fb["idea_id"], []).append(fb)
    prompt = select_prompt.format(
        provider_name=provider.name,
        ideas_json=json.dumps(my_ideas, indent=2),
        feedback_json=json.dumps(feedback_by_idea, indent=2),
        ideas_count=ideas_per_provider,
    )
    result = retry_json_call(
        provider, prompt, schema=SELECTION_SCHEMA,
        context=f"selection ({provider.name})", max_retries=retry_max,
        system=select_system,
    )
    logger.info("  %s selected idea: %s", provider.name, result["selected_idea_id"])
    return provider.name, result

for name, result in _map_concurrently(selection_task, founders, concurrency):
    selections[name] = result
```

PART B — tests/test_pipeline.py:

Add a new test class or method at the bottom of TestPipelineMock:

```python
def test_concurrent_stage1_fires_all_selections(self, tmp_path, monkeypatch):
    """With concurrency=4, all 4 founders' selections are present in output."""
    monkeypatch.chdir(tmp_path)
    run_dir = run_pipeline(
        use_mock=True, concurrency=4, retry_max=1,
        max_iterations=1, ideas_per_provider=2,
    )
    selections = _read_jsonl(run_dir / "stage1_selections.jsonl")
    assert len(selections) == 4
    founder_names = {s["founder_provider"] for s in selections}
    assert founder_names == {"openai", "anthropic", "deepseek", "gemini"}
```
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py -x -v 2>&1 | tail -25</automated>
  </verify>
  <done>selection_task function exists inside run_stage1; _map_concurrently replaces sequential for-loop; all existing and new tests pass.</done>
</task>

</tasks>

<verification>
Run full test suite: `cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/ -v`

All tests pass. Confirm run.py contains two `_map_concurrently` calls inside `run_stage1` (one for idea generation, one for selection) in addition to the already-existing feedback call.
</verification>

<success_criteria>
- Stage 1a: _map_concurrently(idea_gen_task, founders, concurrency) replaces the sequential for-loop
- Stage 1c: _map_concurrently(selection_task, founders, concurrency) replaces the sequential for-loop
- All existing tests pass
- New test test_concurrent_stage1_fires_all_selections passes with concurrency=4
- No behavior change when concurrency=1 (sequential fallback in _map_concurrently)
</success_criteria>

<output>
After completion, create `.planning/phases/01-parallelization/01-01-SUMMARY.md` with:
- What changed in run_stage1 (which loops became concurrent)
- Any gotchas found (closure capture, thread safety of emit)
- Test results
- Key function signatures introduced
</output>
