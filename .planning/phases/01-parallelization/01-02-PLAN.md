---
phase: 01-parallelization
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - vc_agents/pipeline/run.py
  - tests/test_pipeline.py
autonomous: true
requirements:
  - PARA-01
  - PARA-02
must_haves:
  truths:
    - "All 4 founders' Stage 2 build-iterate cycles run at the same time"
    - "Within each round, all advisor reviews fire concurrently, not one-at-a-time"
    - "Final plans dict has one entry per founder, same structure as before"
    - "pytest tests/ -v passes"
  artifacts:
    - path: "vc_agents/pipeline/run.py"
      provides: "Parallelized Stage 2 outer and inner loops"
      contains: "_run_founder_stage2"
    - path: "tests/test_pipeline.py"
      provides: "Test verifying concurrent Stage 2 execution"
  key_links:
    - from: "run_stage2 outer loop"
      to: "_map_concurrently"
      via: "_run_founder_stage2 function extracted and mapped over founders_list"
      pattern: "_map_concurrently.*_run_founder_stage2"
    - from: "advisor review inner loop"
      to: "_map_concurrently"
      via: "review_task function extracted and mapped over advisors"
      pattern: "_map_concurrently.*review_task.*advisors"
---

<objective>
Parallelize Stage 2 in run.py: (1) run all founders' complete build-iterate cycles concurrently by extracting _run_founder_stage2; (2) within each round, run all advisor reviews concurrently with a review_task function.

Purpose: Stage 2 is the longest stage by far (4 founders × 3 rounds × 3 advisor reviews). The outer and inner loops are completely independent within their respective scopes. Running them concurrently produces the 3-4x wall-clock speedup the spec targets.

Output: run.py with parallelized Stage 2; tests verifying concurrent behavior; no change to output file structure or data schemas.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@vc_agents/pipeline/run.py
@tests/test_pipeline.py
@tests/conftest.py
@.planning/phases/01-parallelization/01-01-SUMMARY.md
</context>

<interfaces>
<!-- Key existing interfaces. Read run.py current state after Plan 01 edits. -->

From vc_agents/pipeline/run.py (Stage 2 structure before this plan):

```python
def run_stage2(
    providers: list[BaseProvider],
    selections: dict[str, dict[str, Any]],
    retry_max: int,
    concurrency: int,
    max_iterations: int,
    run_dir: Path,
    emit: EventCallback = noop_callback,
    roles: RoleAssignment | None = None,
    deliberation_enabled: bool = False,
) -> dict[str, dict[str, Any]]:
    """Returns: dict mapping provider_name -> final startup plan."""
    ...
    founders_list = roles.founders if roles is not None else providers
    final_plans: dict[str, dict[str, Any]] = {}
    all_reviews: list[dict[str, Any]] = []  # NOTE: shared across founders — must handle carefully

    # OUTER LOOP — sequential, one founder at a time (lines 512+):
    for founder in founders_list:
        selection = selections[founder.name]
        idea = selection["refined_idea"]
        idea_id = idea["idea_id"]
        ...
        plan = retry_json_call(founder, prompt, schema=STARTUP_PLAN_SCHEMA, ...)
        _write_jsonl(run_dir / f"stage2_{founder.name}_plan_v0.jsonl", [plan])

        # INNER LOOP — sequential, one advisor at a time (lines 532+):
        for round_num in range(1, max_iterations + 1):
            advisors = [a for a in advisors_pool if a.name != founder.name]
            round_reviews: list[dict[str, Any]] = []

            for i, advisor in enumerate(advisors):  # ← PARALLELIZE THIS
                role = ADVISOR_ROLES[(i + round_num - 1) % len(ADVISOR_ROLES)]
                ...
                review = retry_json_call(advisor, prompt, ...)
                round_reviews.append(review)
                all_reviews.append(review)

            # convergence check, deliberation, iterate...
        final_plans[founder.name] = plan

    return final_plans
```

CRITICAL THREADING CONCERN — `all_reviews`:
- `all_reviews` is shared across all founders in the original code
- When parallelizing the outer loop, each founder's task must have its OWN local `founder_reviews` list
- After all founders complete, merge into a single `all_reviews` for the aggregate JSONL write
- The `prev_feedback` lookup inside the inner loop reads from reviews of the SAME founder (same `idea_id`) — so using a per-founder list is correct

CRITICAL THREADING CONCERN — run_dir writes:
- Each founder writes to `stage2_{founder.name}_plan_v*.jsonl` and `stage2_{founder.name}_reviews_round*.jsonl`
- These file names are unique per founder — NO file conflicts during parallel execution
- `stage2_final_plans.jsonl` and `stage2_all_reviews.jsonl` are written AFTER all founders complete

PATTERN FOR INNER LOOP (advisor reviews per round):
```python
# Build a task dict per advisor with all needed context (no shared mutable state)
def review_task(task: dict[str, Any]) -> dict[str, Any]:
    advisor = task["advisor"]
    role = task["role"]
    prompt = advisor_prompt.format(...)
    return retry_json_call(advisor, prompt, schema=ADVISOR_REVIEW_SCHEMA, ...)

advisor_tasks = [
    {"advisor": advisor, "role": ADVISOR_ROLES[(i + round_num - 1) % len(ADVISOR_ROLES)], ...}
    for i, advisor in enumerate(advisors)
]
round_reviews = list(_map_concurrently(review_task, advisor_tasks, concurrency))
```

PATTERN FOR OUTER LOOP (one function per founder):
- Extract `_run_founder_stage2(founder: BaseProvider) -> tuple[str, dict, list]` at module level (not nested inside run_stage2)
- Returns `(founder.name, final_plan, founder_reviews)`
- Must capture all needed variables via a closure OR accept them as parameters via a wrapper lambda
- Recommended: use a `functools.partial` or a local wrapper that closes over the shared read-only config
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Parallelize Stage 2 advisor reviews (inner loop)</name>
  <files>vc_agents/pipeline/run.py</files>
  <behavior>
    - review_task(task_dict) inner function replaces sequential for-loop over advisors
    - task_dict contains: advisor, role, plan, prev_feedback (snapshot), changelog
    - _map_concurrently(review_task, advisor_tasks, concurrency) used for all advisor reviews in a round
    - round_reviews collected from _map_concurrently output; order preserved (executor.map preserves order)
    - all_reviews += round_reviews still works correctly (appended after round completes)
    - Deliberation path unchanged — still runs after round_reviews is collected
    - Existing tests still pass
  </behavior>
  <action>
In run_stage2(), locate the inner advisor review loop (the "for i, advisor in enumerate(advisors):" loop inside the round_num loop).

Replace with:

```python
# Build task dicts — capture everything needed, no shared mutable state
def review_task(task: dict[str, Any]) -> dict[str, Any]:
    adv = task["advisor"]
    role = task["role"]
    prev_section = task["prev_section"]
    changelog_section = task["changelog_section"]
    prompt = advisor_prompt.format(
        provider_name=adv.name,
        advisor_role=role["key"],
        advisor_role_display=role["display"],
        advisor_role_description=role["description"],
        plan_json=json.dumps(task["plan"], indent=2),
        previous_feedback_section=prev_section,
        changelog_section=changelog_section,
    )
    return retry_json_call(
        adv, prompt, schema=ADVISOR_REVIEW_SCHEMA,
        context=f"review ({adv.name}/{task['idea_id']}/round{task['round_num']})",
        max_retries=retry_max,
        system=advisor_system,
    )

advisor_tasks = []
for i, advisor in enumerate(advisors):
    role = ADVISOR_ROLES[(i + round_num - 1) % len(ADVISOR_ROLES)]
    prev_feedback = [
        r for r in all_reviews
        if r["idea_id"] == idea_id and r["reviewer_provider"] == advisor.name
    ]
    prev_section = ""
    if prev_feedback:
        prev_section = (
            "PREVIOUS FEEDBACK YOU GAVE (check if it was addressed):\n"
            + json.dumps(prev_feedback, indent=2)
        )
    changelog = plan.get("changelog", [])
    changelog_section = ""
    if changelog:
        changelog_section = (
            "FOUNDER'S CHANGELOG FROM LAST ITERATION:\n"
            + json.dumps(changelog, indent=2)
        )
    advisor_tasks.append({
        "advisor": advisor,
        "role": role,
        "plan": plan,
        "idea_id": idea_id,
        "round_num": round_num,
        "prev_section": prev_section,
        "changelog_section": changelog_section,
    })

round_reviews = list(_map_concurrently(review_task, advisor_tasks, concurrency))
all_reviews.extend(round_reviews)
```

The `_write_jsonl`, convergence check, deliberation, and iterate steps that follow remain unchanged. Remove the old `round_reviews.append(review)` and `all_reviews.append(review)` lines from the deleted loop body.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py -x -v -k "not concurrent_stage" 2>&1 | tail -25</automated>
  </verify>
  <done>review_task function exists inside run_stage2; _map_concurrently replaces sequential advisor for-loop; round_reviews collected correctly; all existing tests pass.</done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Parallelize Stage 2 outer founder loop + add concurrency test</name>
  <files>vc_agents/pipeline/run.py, tests/test_pipeline.py</files>
  <behavior>
    - _run_founder_stage2 extracted as inner function inside run_stage2 that handles one complete founder cycle
    - Returns (founder_name, final_plan, founder_reviews_list)
    - _map_concurrently(_run_founder_stage2, founders_list, concurrency) replaces outer for-loop
    - final_plans built from tuples; all_reviews accumulated from all founders' review lists after concurrent execution completes
    - stage2_final_plans.jsonl and stage2_all_reviews.jsonl written after all founders complete (unchanged behavior)
    - Test: test_concurrent_stage2_all_founders_complete passes with concurrency=4
  </behavior>
  <action>
PART A — run.py:

Extract the entire per-founder block (from "selection = selections[founder.name]" through "final_plans[founder.name] = plan") into an inner function. This function captures all read-only variables via closure (retry_max, concurrency, run_dir, selections, emit, max_iterations, deliberation_enabled, all the prompt templates, ADVISOR_ROLES, etc.).

Key structure:

```python
all_reviews_lock_free: list[list[dict[str, Any]]] = []  # collects per-founder review lists

def _run_founder_stage2(founder: BaseProvider) -> tuple[str, dict[str, Any], list[dict[str, Any]]]:
    """Run one founder's full Stage 2 cycle. Returns (name, final_plan, reviews)."""
    founder_reviews: list[dict[str, Any]] = []   # local, not shared
    selection = selections[founder.name]
    idea = selection["refined_idea"]
    idea_id = idea["idea_id"]
    logger.info("  Building plan for %s (idea: %s)", founder.name, idea_id)

    # Initial build
    prompt = build_prompt.format(
        provider_name=founder.name,
        idea_json=json.dumps(idea, indent=2),
        context_section="This is your initial plan. Build it from scratch based on the idea above.",
    )
    plan = retry_json_call(
        founder, prompt, schema=STARTUP_PLAN_SCHEMA,
        context=f"build ({founder.name}/{idea_id})", max_retries=retry_max,
        system=build_system,
    )
    _write_jsonl(run_dir / f"stage2_{founder.name}_plan_v0.jsonl", [plan])

    for round_num in range(1, max_iterations + 1):
        logger.info("  Round %d/%d for %s", round_num, max_iterations, founder.name)
        advisors_pool = roles.advisors if roles is not None else providers
        advisors = [a for a in advisors_pool if a.name != founder.name]

        # Inner parallel reviews (already parallelized in Task 1 — copy the review_task pattern here)
        def review_task(task: dict[str, Any]) -> dict[str, Any]:
            adv = task["advisor"]
            role = task["role"]
            prompt_r = advisor_prompt.format(
                provider_name=adv.name,
                advisor_role=role["key"],
                advisor_role_display=role["display"],
                advisor_role_description=role["description"],
                plan_json=json.dumps(task["plan"], indent=2),
                previous_feedback_section=task["prev_section"],
                changelog_section=task["changelog_section"],
            )
            return retry_json_call(
                adv, prompt_r, schema=ADVISOR_REVIEW_SCHEMA,
                context=f"review ({adv.name}/{task['idea_id']}/round{task['round_num']})",
                max_retries=retry_max,
                system=advisor_system,
            )

        advisor_tasks = []
        for i, advisor in enumerate(advisors):
            role = ADVISOR_ROLES[(i + round_num - 1) % len(ADVISOR_ROLES)]
            prev_feedback = [
                r for r in founder_reviews
                if r["idea_id"] == idea_id and r["reviewer_provider"] == advisor.name
            ]
            prev_section = ""
            if prev_feedback:
                prev_section = (
                    "PREVIOUS FEEDBACK YOU GAVE (check if it was addressed):\n"
                    + json.dumps(prev_feedback, indent=2)
                )
            changelog = plan.get("changelog", [])
            changelog_section = ""
            if changelog:
                changelog_section = (
                    "FOUNDER'S CHANGELOG FROM LAST ITERATION:\n"
                    + json.dumps(changelog, indent=2)
                )
            advisor_tasks.append({
                "advisor": advisor, "role": role, "plan": plan,
                "idea_id": idea_id, "round_num": round_num,
                "prev_section": prev_section, "changelog_section": changelog_section,
            })

        round_reviews = list(_map_concurrently(review_task, advisor_tasks, concurrency))
        founder_reviews.extend(round_reviews)

        _write_jsonl(
            run_dir / f"stage2_{founder.name}_reviews_round{round_num}.jsonl",
            round_reviews,
        )

        # Deliberation (unchanged logic — uses round_reviews + founder_reviews as all_reviews)
        if deliberation_enabled and advisors:
            lead = advisors[round_num % len(advisors)]
            delib_prompt = deliberation_user.format(
                reviews_json=json.dumps(round_reviews, indent=2),
                provider_name=lead.name,
            )
            deliberation = retry_json_call(
                lead, delib_prompt, schema=DELIBERATION_SCHEMA,
                context=f"deliberation ({lead.name}/{idea_id}/round{round_num})",
                max_retries=retry_max,
                system=deliberation_system,
            )
            _write_jsonl(
                run_dir / f"stage2_{founder.name}_deliberation_round{round_num}.jsonl",
                [deliberation],
            )
            all_ready = deliberation["overall_readiness"]
            avg_score = deliberation["avg_score"]
            reviews_for_founder = json.dumps(deliberation, indent=2)
        else:
            all_ready = all(r.get("ready_for_pitch", False) for r in round_reviews)
            avg_score = (
                sum(r["readiness_score"] for r in round_reviews) / len(round_reviews)
                if round_reviews else 0.0
            )
            reviews_for_founder = json.dumps(round_reviews, indent=2)

        logger.info("    Avg readiness: %.1f | All ready: %s", avg_score, all_ready)
        emit(PipelineEvent(
            type=EventType.STEP_COMPLETE, stage="stage2", step=f"review_round_{round_num}",
            provider=founder.name, idea_id=idea_id,
            message=f"Round {round_num}: avg={avg_score:.1f}, ready={all_ready}",
            data={"avg_score": avg_score, "all_ready": all_ready, "round": round_num},
        ))

        if round_num >= MIN_ROUNDS_BEFORE_CONVERGENCE and all_ready and avg_score >= 7.5:
            logger.info("    Converged! All advisors signal ready for pitch.")
            break
        if round_num == max_iterations:
            logger.info("    Max iterations reached. Proceeding to pitch anyway.")
            break

        prompt = iterate_prompt.format(
            provider_name=founder.name,
            round_number=round_num,
            plan_json=json.dumps(plan, indent=2),
            reviews_json=reviews_for_founder,
        )
        plan = retry_json_call(
            founder, prompt, schema=STARTUP_PLAN_SCHEMA,
            context=f"iterate ({founder.name}/{idea_id}/round{round_num})",
            max_retries=retry_max,
            system=iterate_system,
        )
        _write_jsonl(
            run_dir / f"stage2_{founder.name}_plan_v{round_num}.jsonl",
            [plan],
        )

    return founder.name, plan, founder_reviews

# Run all founders concurrently
final_plans: dict[str, dict[str, Any]] = {}
all_reviews: list[dict[str, Any]] = []
for f_name, f_plan, f_reviews in _map_concurrently(_run_founder_stage2, founders_list, concurrency):
    final_plans[f_name] = f_plan
    all_reviews.extend(f_reviews)
```

Then keep the existing aggregate writes and emit unchanged:
```python
_write_jsonl(run_dir / "stage2_final_plans.jsonl", list(final_plans.values()))
_write_jsonl(run_dir / "stage2_all_reviews.jsonl", all_reviews)
emit(PipelineEvent(type=EventType.STAGE_COMPLETE, stage="stage2", ...))
return final_plans
```

Note: After this refactor, the old Task 1 (inner advisor parallelization) is now ABSORBED into _run_founder_stage2. The separate intermediate state from Task 1 will be replaced by the cleaner version here. Review the current state of run.py carefully before editing.

PART B — tests/test_pipeline.py:

Add to TestPipelineMock class:

```python
def test_concurrent_stage2_all_founders_complete(self, tmp_path, monkeypatch):
    """With concurrency=4, all 4 founders complete Stage 2 and produce final plans."""
    monkeypatch.chdir(tmp_path)
    run_dir = run_pipeline(
        use_mock=True, concurrency=4, retry_max=1,
        max_iterations=2, ideas_per_provider=2,
    )
    plans = _read_jsonl(run_dir / "stage2_final_plans.jsonl")
    assert len(plans) == 4
    founder_names = {p["founder_provider"] for p in plans}
    assert founder_names == {"openai", "anthropic", "deepseek", "gemini"}
    # Each founder should have written at least a v0 plan file
    for name in ["openai", "anthropic", "deepseek", "gemini"]:
        assert (run_dir / f"stage2_{name}_plan_v0.jsonl").exists()
```
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/ -v 2>&1 | tail -30</automated>
  </verify>
  <done>_run_founder_stage2 inner function exists inside run_stage2; _map_concurrently maps it over founders_list; final_plans dict correct; all_reviews aggregated from per-founder lists; all tests pass including new concurrent Stage 2 test.</done>
</task>

</tasks>

<verification>
Run full test suite: `cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/ -v`

All tests pass. Confirm:
- run_stage2 contains _run_founder_stage2 inner function
- _run_founder_stage2 contains review_task inner function
- Both use _map_concurrently
- stage2_final_plans.jsonl contains 4 entries in a mock run
- stage2_all_reviews.jsonl is non-empty
</verification>

<success_criteria>
- Stage 2 outer loop: _map_concurrently(_run_founder_stage2, founders_list, concurrency) replaces sequential for-loop
- Stage 2 inner loop: _map_concurrently(review_task, advisor_tasks, concurrency) replaces sequential advisor for-loop
- Per-founder file writes are unique (no conflicts): stage2_{name}_plan_v*.jsonl, stage2_{name}_reviews_round*.jsonl
- Aggregate files written after all concurrent work completes: stage2_final_plans.jsonl, stage2_all_reviews.jsonl
- all_reviews is thread-safe (per-founder lists merged after _map_concurrently returns)
- All existing tests pass; new concurrent stage 2 test passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-parallelization/01-02-SUMMARY.md` with:
- How _run_founder_stage2 is structured
- How all_reviews thread-safety is handled (per-founder local lists)
- Any deviations from the plan (e.g., if Task 1's intermediate state was already present)
- Test results
</output>
