---
phase: 01-backend-fixes
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - vc_agents/web/server.py
  - vc_agents/pipeline/run.py
autonomous: true
requirements:
  - BUG-03
  - BUG-04

must_haves:
  truths:
    - "POST /api/runs appears in /docs with a RunConfig JSON schema showing use_mock, models, base_urls, api_keys fields"
    - "Submitting a malformed body to POST /api/runs returns 422 with field-level error details"
    - "run_pipeline() accepts slot3_base_url and slot4_base_url parameters that override the hardcoded DEEPSEEK_BASE_URL and GEMINI_BASE_URL env vars"
    - "server.py reads config.base_urls and forwards slot3/slot4 values to run_pipeline() so a request body with base_urls overrides where the OpenAI-compatible client connects"
  artifacts:
    - path: "vc_agents/web/server.py"
      provides: "RunConfig Pydantic model and updated create_run endpoint"
      contains: "class RunConfig(BaseModel)"
    - path: "vc_agents/pipeline/run.py"
      provides: "slot3_base_url and slot4_base_url parameters in run_pipeline signature"
      contains: "slot3_base_url"
  key_links:
    - from: "server.py create_run(config: RunConfig)"
      to: "run_pipeline(slot3_base_url=..., slot4_base_url=...)"
      via: "_run_in_thread passing config.base_urls values"
      pattern: "config\\.base_urls"
    - from: "run_pipeline(slot3_base_url, slot4_base_url)"
      to: "OpenAICompatibleChat(base_url=slot3_base_url)"
      via: "hardcoded-fallback provider init block in run_pipeline"
      pattern: "slot3_base_url"
---

<objective>
Replace the fragile `dict[str, Any]` body annotation on `POST /api/runs` with a Pydantic `RunConfig` model (BUG-03), and wire the `base_urls` fields from that model through `server.py` → `run_pipeline()` → `OpenAICompatibleChat` so Slots 3 & 4 use the dashboard-supplied base URL instead of hardcoded env vars (BUG-04).

Purpose: These two bugs are coupled — RunConfig provides the `base_urls` field that BUG-04 needs to forward. After this plan, the API validates input correctly, documents itself via `/docs`, and actually connects Slots 3 & 4 to whichever provider the user configured.
Output: Updated server.py with RunConfig model + updated run.py with slot URL params.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-fixes/01-01-SUMMARY.md
</context>

<interfaces>
<!-- Key code sections the executor must understand before editing. -->

From vc_agents/web/server.py (current POST /api/runs, lines 131-156):
```python
@app.post("/api/runs")
async def create_run(config: dict[str, Any] | None = None) -> JSONResponse:
    """Launch a new pipeline run."""
    config = config or {}
    run_id = f"run_{uuid.uuid4().hex[:12]}"

    with _runs_lock:
        _runs[run_id] = {
            "run_id": run_id,
            "status": "starting",
            "config": config,
            "events": [],
            ...
        }

    loop = asyncio.get_running_loop()
    thread = threading.Thread(
        target=_run_in_thread, args=(run_id, config, loop), daemon=True,
    )
    thread.start()
    return JSONResponse({"run_id": run_id, "status": "starting"})
```

From vc_agents/web/server.py (_run_in_thread, lines 94-123):
```python
def _run_in_thread(run_id: str, config: dict[str, Any], loop: asyncio.AbstractEventLoop) -> None:
    """Run the pipeline in a background thread."""
    emit = _make_emit(run_id, loop)
    ...
    try:
        run_dir = run_pipeline(
            use_mock=config.get("use_mock", True),
            concurrency=config.get("concurrency", 1),
            retry_max=config.get("retry_max", 3),
            max_iterations=config.get("max_iterations", 3),
            ideas_per_provider=config.get("ideas_per_provider", 5),
            sector_focus=sector,
            deliberation_enabled=config.get("deliberation_enabled", False),
            emit=emit,
            provider_config=config,
        )
```

From vc_agents/pipeline/run.py (run_pipeline signature, lines 721-733):
```python
def run_pipeline(
    use_mock: bool,
    concurrency: int,
    retry_max: int,
    max_iterations: int = 3,
    ideas_per_provider: int = 5,
    sector_focus: str = "",
    emit: EventCallback = noop_callback,
    provider_config: dict[str, Any] | None = None,
    resume_dir: Path | None = None,
    roles_config: dict[str, Any] | None = None,
    deliberation_enabled: bool = False,
) -> Path:
```

From vc_agents/pipeline/run.py (hardcoded fallback provider init, lines 763-774):
```python
            OpenAICompatibleChat(
                name="deepseek",
                model=models.get("deepseek") or os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner"),
                base_url=os.getenv("DEEPSEEK_BASE_URL"),
                api_key=api_keys.get("DEEPSEEK_API_KEY"),
            ),
            OpenAICompatibleChat(
                name="gemini",
                api_key_env=os.getenv("GEMINI_API_KEY_ENV", "GEMINI_API_KEY"),
                model=models.get("gemini") or os.getenv("GEMINI_MODEL", "gemini-3-pro-preview"),
                base_url=os.getenv("GEMINI_BASE_URL"),
                api_key=api_keys.get("GEMINI_API_KEY"),
            ),
```
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Add RunConfig Pydantic model and update POST /api/runs (BUG-03)</name>
  <files>vc_agents/web/server.py</files>
  <action>
    **Step 1 — Add Pydantic import:**
    In the imports block, add `from pydantic import BaseModel` after the existing FastAPI imports. The import block currently has `from fastapi import FastAPI, WebSocket, WebSocketDisconnect` — add pydantic on the next logical line.

    **Step 2 — Define RunConfig model:**
    After the `from vc_agents.pipeline.run import _load_jsonl, run_pipeline` import line and before the `load_dotenv()` call, add the following Pydantic model:

    ```python
    class RunConfig(BaseModel):
        use_mock: bool = True
        max_iterations: int = 3
        ideas_per_provider: int = 5
        sector_focus: str = ""
        concurrency: int = 1
        retry_max: int = 3
        deliberation_enabled: bool = False
        models: dict[str, str] = {}
        base_urls: dict[str, str] = {}
        api_keys: dict[str, str] = {}
    ```

    **Step 3 — Update create_run endpoint signature:**
    Change the `POST /api/runs` handler from:
    ```python
    async def create_run(config: dict[str, Any] | None = None) -> JSONResponse:
        config = config or {}
    ```
    to:
    ```python
    async def create_run(config: RunConfig = RunConfig()) -> JSONResponse:
    ```
    Remove the `config = config or {}` line — it is no longer needed.

    **Step 4 — Update config storage:**
    When storing config in `_runs[run_id]["config"]`, change:
    ```python
    "config": config,
    ```
    to:
    ```python
    "config": config.model_dump(),
    ```
    This keeps the stored value as a plain dict for JSON serialization.

    **Step 5 — Update _run_in_thread call:**
    The thread is launched with `args=(run_id, config, loop)`. Pass `config.model_dump()` instead so `_run_in_thread` still receives a dict (backward compatible):
    ```python
    thread = threading.Thread(
        target=_run_in_thread, args=(run_id, config.model_dump(), loop), daemon=True,
    )
    ```

    Do NOT change `_run_in_thread` itself — it still operates on a dict and forwards to `run_pipeline`. That forwarding is extended in Task 2.

    Do NOT remove `from typing import Any` — it is used by other endpoints (`get_estimate`, `_run_in_thread`, etc.).
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -c "from vc_agents.web.server import app, RunConfig; c = RunConfig(); print('OK', c.use_mock)"</automated>
  </verify>
  <done>
    `RunConfig` is importable and instantiates with defaults. `POST /api/runs` handler accepts `config: RunConfig`. The `_runs` dict stores `config.model_dump()` (a plain dict). Server module imports without error.
  </done>
</task>

<task type="auto">
  <name>Task 2: Forward slot base URLs through server.py and run_pipeline() (BUG-04)</name>
  <files>vc_agents/web/server.py, vc_agents/pipeline/run.py</files>
  <action>
    **In vc_agents/web/server.py — update _run_in_thread:**

    The `_run_in_thread` function receives `config` as a dict (via `config.model_dump()` after Task 1). Add two base URL extractions and pass them to `run_pipeline`:

    ```python
    def _run_in_thread(run_id: str, config: dict[str, Any], loop: asyncio.AbstractEventLoop) -> None:
        """Run the pipeline in a background thread."""
        emit = _make_emit(run_id, loop)
        with _runs_lock:
            _runs[run_id]["status"] = "running"
            _runs[run_id]["started_at"] = time.time()

        sector = config.get("sector_focus", "")
        base_urls = config.get("base_urls", {})
        slot3_base_url = base_urls.get("slot3") or base_urls.get("deepseek") or os.getenv("SLOT3_BASE_URL", "https://api.deepseek.com/v1")
        slot4_base_url = base_urls.get("slot4") or base_urls.get("gemini") or os.getenv("SLOT4_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/openai")

        try:
            run_dir = run_pipeline(
                use_mock=config.get("use_mock", True),
                concurrency=config.get("concurrency", 1),
                retry_max=config.get("retry_max", 3),
                max_iterations=config.get("max_iterations", 3),
                ideas_per_provider=config.get("ideas_per_provider", 5),
                sector_focus=sector,
                deliberation_enabled=config.get("deliberation_enabled", False),
                emit=emit,
                provider_config=config,
                slot3_base_url=slot3_base_url,
                slot4_base_url=slot4_base_url,
            )
    ```

    The lookup order `base_urls.get("slot3") or base_urls.get("deepseek")` handles both the new slot-keyed format the dashboard will send and the legacy provider-name-keyed format. Falls back to `SLOT3_BASE_URL` env var (renamed from `DEEPSEEK_BASE_URL` per spec).

    **In vc_agents/pipeline/run.py — update run_pipeline signature:**

    Add `slot3_base_url` and `slot4_base_url` parameters to `run_pipeline()`:

    ```python
    def run_pipeline(
        use_mock: bool,
        concurrency: int,
        retry_max: int,
        max_iterations: int = 3,
        ideas_per_provider: int = 5,
        sector_focus: str = "",
        emit: EventCallback = noop_callback,
        provider_config: dict[str, Any] | None = None,
        resume_dir: Path | None = None,
        roles_config: dict[str, Any] | None = None,
        deliberation_enabled: bool = False,
        slot3_base_url: str = "https://api.deepseek.com/v1",
        slot4_base_url: str = "https://generativelanguage.googleapis.com/v1beta/openai",
    ) -> Path:
    ```

    **In vc_agents/pipeline/run.py — update hardcoded fallback provider init:**

    In the `else` branch of `if yaml_config.get("providers"):` (the hardcoded fallback, lines ~763-774), replace:

    ```python
                OpenAICompatibleChat(
                    name="deepseek",
                    model=models.get("deepseek") or os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner"),
                    base_url=os.getenv("DEEPSEEK_BASE_URL"),
                    api_key=api_keys.get("DEEPSEEK_API_KEY"),
                ),
                OpenAICompatibleChat(
                    name="gemini",
                    api_key_env=os.getenv("GEMINI_API_KEY_ENV", "GEMINI_API_KEY"),
                    model=models.get("gemini") or os.getenv("GEMINI_MODEL", "gemini-3-pro-preview"),
                    base_url=os.getenv("GEMINI_BASE_URL"),
                    api_key=api_keys.get("GEMINI_API_KEY"),
                ),
    ```

    with:

    ```python
                OpenAICompatibleChat(
                    name="deepseek",
                    model=models.get("deepseek") or os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner"),
                    base_url=slot3_base_url,
                    api_key=api_keys.get("DEEPSEEK_API_KEY"),
                ),
                OpenAICompatibleChat(
                    name="gemini",
                    api_key_env=os.getenv("GEMINI_API_KEY_ENV", "GEMINI_API_KEY"),
                    model=models.get("gemini") or os.getenv("GEMINI_MODEL", "gemini-3-pro-preview"),
                    base_url=slot4_base_url,
                    api_key=api_keys.get("GEMINI_API_KEY"),
                ),
    ```

    The `slot3_base_url` and `slot4_base_url` default values already cover the old hardcoded env var values, so existing deployments using `.env` with `DEEPSEEK_BASE_URL` / `GEMINI_BASE_URL` will need to rename those to `SLOT3_BASE_URL` / `SLOT4_BASE_URL` (the defaults in run_pipeline now encode those fallback URLs directly). The old env vars are no longer read.

    Note: The `_build_providers_from_config` path (lines ~69-95) reads `base_url_env` from the YAML config and calls `os.getenv(entry["base_url_env"])` — that path is NOT changed by this fix. The `slot3_base_url` / `slot4_base_url` params only affect the hardcoded fallback branch.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -c "import inspect; from vc_agents.pipeline.run import run_pipeline; sig = inspect.signature(run_pipeline); print('slot3_base_url' in sig.parameters, 'slot4_base_url' in sig.parameters)"</automated>
  </verify>
  <done>
    `run_pipeline` signature contains `slot3_base_url` and `slot4_base_url` parameters with correct defaults. `_run_in_thread` extracts `base_urls` from the config dict and passes the resolved values. Both modules import without error. The verification script prints `True True`.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `python -c "from vc_agents.web.server import app, RunConfig; c = RunConfig(); print(c.base_urls)"` prints `{}`
2. `python -c "import inspect; from vc_agents.pipeline.run import run_pipeline; print(list(inspect.signature(run_pipeline).parameters.keys()))"` shows `slot3_base_url` and `slot4_base_url`
3. `python -c "from vc_agents.web.server import app; from fastapi.testclient import TestClient; c = TestClient(app); r = c.post('/api/runs', json={'use_mock': True}); print(r.status_code)"` returns 200
4. `python -c "from vc_agents.web.server import app; from fastapi.testclient import TestClient; c = TestClient(app); r = c.post('/api/runs', json={'use_mock': 'not_a_bool'}); print(r.status_code)"` returns 422
</verification>

<success_criteria>
1. `POST /api/runs` is typed with `RunConfig` — FastAPI parses and validates the JSON body, returns 422 on schema violations
2. `run_pipeline()` accepts `slot3_base_url` and `slot4_base_url` — the hardcoded `os.getenv("DEEPSEEK_BASE_URL")` and `os.getenv("GEMINI_BASE_URL")` calls are replaced with the parameter values
3. `_run_in_thread` extracts `config["base_urls"]` and forwards resolved URLs to `run_pipeline` — a request with `base_urls: {"deepseek": "https://api.mistral.ai/v1"}` results in the Slot 3 `OpenAICompatibleChat` client pointing to that URL
</success_criteria>

<output>
After completion, create `.planning/phases/01-backend-fixes/01-02-SUMMARY.md` using the summary template.
</output>
