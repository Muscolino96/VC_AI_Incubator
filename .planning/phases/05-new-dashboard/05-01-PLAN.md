---
phase: 05-new-dashboard
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - vc_agents/web/dashboard.html
  - vc_agents/web/server.py
autonomous: true
requirements:
  - DASH-01
  - DASH-02

must_haves:
  truths:
    - "Visiting http://localhost:8000 serves the new premium dashboard (serif wordmark, gold token, Team Builder, slot strip)"
    - "Starting a mock run from the new dashboard triggers a pipeline run (base_urls payload is accepted without 400 error)"
    - "The /api/runs/{id}/results endpoint returns a token_usage key with per-provider token data when token_usage.json exists"
  artifacts:
    - path: "vc_agents/web/dashboard.html"
      provides: "New premium dashboard — Team Builder, slot strip, conviction rings, votes bar"
      contains: "SLOTS"
    - path: "vc_agents/web/server.py"
      provides: "Results endpoint returns token_usage; RunConfig already accepts base_urls"
      contains: "token_usage"
  key_links:
    - from: "docs/dashboard.html startRun()"
      to: "POST /api/runs"
      via: "config.base_urls payload"
      pattern: "base_urls.*deepseek"
    - from: "vc_agents/web/server.py get_results()"
      to: "token_usage.json"
      via: "Path.read_text + json.loads"
      pattern: "token_usage"
---

<objective>
Deploy the new premium dashboard and ensure the server returns token usage data.

Purpose: Replace the old dashboard.html with the fully-built new version from docs/dashboard.html, which has the Team Builder UI, slot strip, conviction rings, and base_urls passthrough already wired. Also patch the results endpoint to serve token_usage.json alongside the JSONL files (needed by Plan 02).

Output:
- vc_agents/web/dashboard.html replaced with docs/dashboard.html content
- server.py get_results() reads token_usage.json when present and includes it in the response
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@docs/dashboard.html
@vc_agents/web/server.py

**CRITICAL PROJECT RULE (from CLAUDE.md):** Do NOT simplify, flatten, or "clean up" the CSS/JS. The design is intentional. Copy docs/dashboard.html verbatim — no reformatting, no style changes, no JS restructuring.

**Key facts:**
- docs/dashboard.html is 1358 lines. It already sends base_urls in startRun(). Copy it exactly.
- server.py RunConfig already has `base_urls: dict[str, str] = {}` — no change needed there.
- server.py _run_in_thread already extracts base_urls and passes slot3_base_url/slot4_base_url to run_pipeline() — no change needed there.
- The only server.py change needed: get_results() currently only globs *.jsonl files. token_usage.json is a JSON file and must be read separately.

**token_usage.json structure** (written by run_pipeline):
```json
{
  "openai":    { "input_tokens": 1234, "output_tokens": 5678 },
  "anthropic": { "input_tokens": 2345, "output_tokens": 6789 },
  "deepseek":  { "input_tokens": 3456, "output_tokens": 7890 },
  "gemini":    { "input_tokens": 4567, "output_tokens": 8901 }
}
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Copy docs/dashboard.html to vc_agents/web/dashboard.html verbatim</name>
  <files>vc_agents/web/dashboard.html</files>
  <action>
    Read the full contents of docs/dashboard.html (1358 lines). Write those contents exactly to vc_agents/web/dashboard.html, completely replacing the old dashboard.

    CRITICAL: Do NOT modify, reformat, minify, simplify, or restructure the HTML, CSS, or JS in any way. The design is intentional. The file must be byte-for-byte identical to docs/dashboard.html except for this one specific change:

    In the startRun() function, the config object already has:
    ```js
    base_urls: {
      deepseek: state.baseUrls[2] || '',
      gemini:   state.baseUrls[3] || '',
    },
    ```
    This is correct. Do NOT change it.

    Also add `deliberation_enabled: false` to the config object in startRun() if it is not already present. Check first — only add if missing. Place it after `retry_max: 3`.

    Verify after writing: the file must contain the string "SLOTS" (the model catalog), "wordmark-vc" (premium header), and "base_urls" (the payload field).
  </action>
  <verify>
    <automated>python -c "
content = open('vc_agents/web/dashboard.html', encoding='utf-8').read()
assert 'SLOTS' in content, 'SLOTS array missing'
assert 'wordmark-vc' in content, 'Premium header missing'
assert 'base_urls' in content, 'base_urls payload missing'
assert 'team-builder' in content, 'Team Builder UI missing'
assert 'conviction-ring' in content, 'Conviction ring SVG missing'
print('dashboard.html OK')
"
    </automated>
  </verify>
  <done>vc_agents/web/dashboard.html contains the new premium dashboard with Team Builder, slot strip, and base_urls in the startRun() payload</done>
</task>

<task type="auto">
  <name>Task 2: Patch get_results() to serve token_usage.json</name>
  <files>vc_agents/web/server.py</files>
  <action>
    In the get_results() function in server.py, after the CSV portfolio_report block, add a block that reads token_usage.json if it exists:

    ```python
    # Read token usage if it exists
    token_usage_path = run_dir / "token_usage.json"
    if token_usage_path.exists():
        results["token_usage"] = json.loads(token_usage_path.read_text(encoding="utf-8"))
    ```

    Place this block immediately after the CSV block (after the `if csv_path.exists():` block closes). Do NOT modify any other part of the function. Do NOT change RunConfig, _run_in_thread, or any other function.

    The resulting results dict will now include a "token_usage" key when that file exists, making it accessible to the dashboard's fetch call.
  </action>
  <verify>
    <automated>python -c "
import ast, sys
src = open('vc_agents/web/server.py', encoding='utf-8').read()
assert 'token_usage_path' in src, 'token_usage patch missing'
assert 'token_usage.json' in src, 'filename reference missing'
# Ensure file parses cleanly
ast.parse(src)
print('server.py syntax OK, token_usage patch present')
"
    </automated>
  </verify>
  <done>GET /api/runs/{id}/results returns a "token_usage" key containing per-provider input/output token counts when a token_usage.json file exists in the run directory</done>
</task>

</tasks>

<verification>
After both tasks:
1. Start server: `python -m vc_agents.web.server` — confirm it starts without errors
2. Visit http://localhost:8000 — confirm the new premium dashboard loads (gold serif wordmark, 4-slot Team Builder panel, slot strip visible)
3. Run pytest: `pytest tests/ -v` — all existing tests must still pass (server.py change is additive only)
</verification>

<success_criteria>
- vc_agents/web/dashboard.html is the new 1358-line premium dashboard
- http://localhost:8000 shows the Team Builder, slot strip, and gold aesthetic
- /api/runs/{id}/results includes "token_usage" key for completed runs with token_usage.json
- pytest tests/ -v passes (zero regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/05-new-dashboard/05-01-SUMMARY.md` summarizing:
- What was done (files changed, lines modified)
- Verification results (pytest output, manual check)
- Any deviations from plan
</output>
