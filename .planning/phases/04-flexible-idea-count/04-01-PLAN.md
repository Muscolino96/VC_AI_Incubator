---
phase: 04-flexible-idea-count
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - vc_agents/pipeline/prompts/select_prompt.txt
  - vc_agents/pipeline/run.py
  - tests/test_pipeline.py
autonomous: true
requirements: [IDEA-01, IDEA-02, IDEA-03, IDEA-04]

must_haves:
  truths:
    - "--ideas-per-provider 1 completes a full run without making a selection LLM call"
    - "The single idea is auto-selected and passed to Stage 2 intact"
    - "The Stage 1 feedback step runs even when ideas_per_provider=1"
    - "--ideas-per-provider 2 makes a selection LLM call with adapted prompt wording (not 'pick from 5')"
    - "Any integer >= 1 completes without prompt errors or broken JSON schemas"
  artifacts:
    - path: "vc_agents/pipeline/prompts/select_prompt.txt"
      provides: "Count-aware selection prompt (adapts wording to ideas_count)"
      contains: "{ideas_count}"
    - path: "vc_agents/pipeline/run.py"
      provides: "Modified run_stage1() with count=1 bypass and count-aware selection"
      contains: "ideas_per_provider == 1"
    - path: "tests/test_pipeline.py"
      provides: "TestFlexibleIdeas tests covering IDEA-01 through IDEA-04"
      contains: "TestFlexibleIdeas"
  key_links:
    - from: "run_stage1() selection_task branch"
      to: "auto-selection synthetic result"
      via: "ideas_per_provider == 1 guard"
      pattern: "if ideas_per_provider == 1"
    - from: "select_prompt.txt SYSTEM section"
      to: "ideas_count-aware language"
      via: "{ideas_count} format variable already present"
      pattern: "ideas_count.*idea"
---

<objective>
Support `--ideas-per-provider 1` and `2` with correct behavior: count=1 auto-selects without an LLM call; count=2 adapts the selection prompt wording; feedback always runs; any count >= 1 works cleanly.

Purpose: Operators sometimes want to test the build-iterate-pitch flow with a predetermined idea, without waiting for ideation overhead. They also want smaller runs (2 ideas) that still do proper selection.

Output: Modified `run_stage1()`, updated `select_prompt.txt`, and a `TestFlexibleIdeas` test class.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<interfaces>
<!-- Key interfaces the executor needs. Extracted from codebase. -->

From vc_agents/pipeline/run.py — run_stage1() signature:
```python
def run_stage1(
    providers: list[BaseProvider],
    ideas_per_provider: int,
    retry_max: int,
    concurrency: int,
    run_dir: Path,
    sector_focus: str = "",
    emit: EventCallback = noop_callback,
    roles: RoleAssignment | None = None,
) -> dict[str, dict[str, Any]]:
    """Returns: dict mapping provider_name -> selection result (with refined_idea)."""
```

From vc_agents/pipeline/run.py — current selection_task (lines ~513-535):
```python
def selection_task(provider: BaseProvider) -> tuple[str, dict[str, Any]]:
    my_ideas = all_ideas[provider.name]
    my_idea_ids = {idea["idea_id"] for idea in my_ideas}
    my_feedback = [f for f in all_feedback if f["idea_id"] in my_idea_ids]
    feedback_by_idea: dict[str, list[dict[str, Any]]] = {}
    for fb in my_feedback:
        feedback_by_idea.setdefault(fb["idea_id"], []).append(fb)
    prompt = select_prompt.format(
        provider_name=provider.name,
        ideas_json=json.dumps(my_ideas, indent=2),
        feedback_json=json.dumps(feedback_by_idea, indent=2),
        ideas_count=ideas_per_provider,
    )
    result = retry_json_call(
        provider, prompt, schema=SELECTION_SCHEMA,
        context=f"selection ({provider.name})", max_retries=retry_max,
        system=select_system,
    )
    logger.info("  %s selected idea: %s", provider.name, result["selected_idea_id"])
    return provider.name, result

for name, result in _map_concurrently(selection_task, founders, concurrency):
    selections[name] = result

_write_jsonl(run_dir / "stage1_selections.jsonl", list(selections.values()))
```

From vc_agents/pipeline/run.py — Step 1c comment (line 509):
```python
# --- Step 1c: Each founder selects best idea (parallel) ---
```

From vc_agents/pipeline/prompts/select_prompt.txt — current content:
```
---SYSTEM---
You are a startup founder who proposed {ideas_count} ideas and received detailed feedback
from 3 experienced advisors. Now you must pick the single best idea to pursue and commit to building.
...
---USER---
YOUR {ideas_count} IDEAS:
{ideas_json}

FEEDBACK FROM ALL ADVISORS (grouped by idea):
{feedback_json}
...
Return a JSON object with these exact fields:
- selected_idea_id (string, the idea_id of your chosen idea)
- founder_provider (string, MUST be exactly: {provider_name})
- reasoning (string, 3-5 sentences ...)
- refined_idea (object, ...)
```

From vc_agents/schemas — SELECTION_SCHEMA requires:
```python
# Fields: selected_idea_id, founder_provider, reasoning, refined_idea
# The auto-selection result for count=1 MUST conform to this same schema.
```

From tests/test_pipeline.py — existing test patterns (for style reference):
```python
class TestResume:
    def test_...(self, tmp_path, monkeypatch):
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=2,
        )
        ...assert...
```
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Adapt select_prompt.txt and modify run_stage1() for flexible idea counts</name>
  <files>
    vc_agents/pipeline/prompts/select_prompt.txt
    vc_agents/pipeline/run.py
  </files>
  <behavior>
    - Test 1: run_pipeline(use_mock=True, ideas_per_provider=1) completes without error
    - Test 2: With ideas_per_provider=1, stage1_selections.jsonl is written with 4 records (one per founder)
    - Test 3: With ideas_per_provider=1, each selection record has selected_idea_id, founder_provider, reasoning, refined_idea
    - Test 4: With ideas_per_provider=1, feedback still runs (stage1_feedback.jsonl exists and is non-empty)
    - Test 5: With ideas_per_provider=2, stage1_selections.jsonl has 4 records (LLM call was made)
    - Test 6: run_pipeline(use_mock=True, ideas_per_provider=3) completes without error (regression)
  </behavior>
  <action>
**Part A — Update select_prompt.txt:**

Read the current file first. Then rewrite ONLY the SYSTEM section to be count-aware. Replace the hardcoded phrase "pick the single best idea" with dynamic wording based on `{ideas_count}`:

```
---SYSTEM---
You are a startup founder who proposed {ideas_count} idea(s) and received detailed feedback from 3 experienced advisors. Now you must select the best idea to pursue and commit to building.

DECISION PROCESS:
1. Review all feedback carefully. Look for patterns -- if multiple advisors flag the same weakness, take it seriously.
2. Consider which idea has the highest upside AND the most addressable weaknesses.
3. Don't just pick the highest-scored idea. Sometimes a lower-scored idea with a clear path to fixing its weaknesses is better.
4. Factor in what you can realistically build and validate in 6 months with a small team.
---USER---
YOUR {ideas_count} IDEA(S):
{ideas_json}

FEEDBACK FROM ALL ADVISORS (grouped by idea):
{feedback_json}

AFTER SELECTING:
- Incorporate the best suggestions from the advisors into a refined version of the idea
- Update any fields that the feedback revealed should change (market focus, positioning, etc.)

Return a JSON object with these exact fields:
- selected_idea_id (string, the idea_id of your chosen idea)
- founder_provider (string, MUST be exactly: {provider_name})
- reasoning (string, 2-5 sentences explaining your selection, referencing specific feedback)
- refined_idea (object, the full idea card with improvements applied -- must contain all idea card fields: idea_id, title, summary, target_customer, why_now, market_size_estimate, unfair_advantage, proposer_provider)

Return ONLY valid JSON. No markdown fences, no explanatory text before or after.
```

Key changes: "single best idea" → "best idea to pursue", "YOUR {ideas_count} IDEAS:" → "YOUR {ideas_count} IDEA(S):", reasoning is "2-5 sentences" (was 3-5, to be less demanding for single-idea case).

**Part B — Modify run_stage1() in run.py:**

In the `# --- Step 1c: Each founder selects best idea (parallel) ---` block, replace the current selection logic with a branch:

```python
# --- Step 1c: Each founder selects best idea (or auto-selects if only 1) ---
logger.info("Step 1c: Founders select best idea (ideas_per_provider=%d)", ideas_per_provider)
selections: dict[str, dict[str, Any]] = {}

if ideas_per_provider == 1:
    # Auto-select: skip the LLM call. Build a synthetic selection result
    # conforming to SELECTION_SCHEMA from the single idea + its feedback.
    logger.info("  ideas_per_provider=1: auto-selecting single idea for each founder")
    for provider in founders:
        my_ideas = all_ideas[provider.name]
        idea = my_ideas[0]
        # Collect feedback for this idea to build a brief reasoning string
        my_feedback = [f for f in all_feedback if f["idea_id"] == idea["idea_id"]]
        avg_score = (
            sum(f.get("score", 5) for f in my_feedback) / len(my_feedback)
            if my_feedback else 5.0
        )
        synthetic = {
            "selected_idea_id": idea["idea_id"],
            "founder_provider": provider.name,
            "reasoning": (
                f"Auto-selected the only idea '{idea['idea_id']}' (single-idea mode). "
                f"Advisor feedback average score: {avg_score:.1f}. "
                f"Proceeding with this idea incorporating advisor suggestions."
            ),
            "refined_idea": dict(idea),  # carry the idea through unchanged
        }
        selections[provider.name] = synthetic
        logger.info("  %s auto-selected idea: %s", provider.name, idea["idea_id"])
else:
    # Normal path: each founder calls the LLM to select (parallel)
    def selection_task(provider: BaseProvider) -> tuple[str, dict[str, Any]]:
        my_ideas = all_ideas[provider.name]
        my_idea_ids = {idea["idea_id"] for idea in my_ideas}
        my_feedback = [f for f in all_feedback if f["idea_id"] in my_idea_ids]
        feedback_by_idea: dict[str, list[dict[str, Any]]] = {}
        for fb in my_feedback:
            feedback_by_idea.setdefault(fb["idea_id"], []).append(fb)
        prompt = select_prompt.format(
            provider_name=provider.name,
            ideas_json=json.dumps(my_ideas, indent=2),
            feedback_json=json.dumps(feedback_by_idea, indent=2),
            ideas_count=ideas_per_provider,
        )
        result = retry_json_call(
            provider, prompt, schema=SELECTION_SCHEMA,
            context=f"selection ({provider.name})", max_retries=retry_max,
            system=select_system,
        )
        logger.info("  %s selected idea: %s", provider.name, result["selected_idea_id"])
        return provider.name, result

    for name, result in _map_concurrently(selection_task, founders, concurrency):
        selections[name] = result
```

The rest of Step 1c (writing stage1_selections.jsonl, emitting STAGE_COMPLETE) remains unchanged.

IMPORTANT: The feedback step (Step 1b) is NOT conditional on ideas_per_provider — it must always run. Do not add any guard around Step 1b. The spec says "Feedback CAN still run (useful as early validation)" — this means it always runs, not optionally.

Also verify: the `_write_jsonl(run_dir / "stage1_selections.jsonl", ...)` call and the `emit(STAGE_COMPLETE)` call are outside both branches (they come after the if/else block), so both code paths produce the same output files.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py -v -k "not para06 and not slow" -x 2>&1 | tail -20</automated>
  </verify>
  <done>
    - select_prompt.txt no longer says "single best idea" and correctly uses "{ideas_count} IDEA(S)"
    - run_stage1() has an `if ideas_per_provider == 1` branch that skips the LLM call and builds a synthetic SELECTION_SCHEMA-conforming result
    - The auto-selection result has all required fields: selected_idea_id, founder_provider, reasoning, refined_idea
    - Feedback (Step 1b) still runs unconditionally for count=1
    - The `else` branch contains the original LLM selection logic unchanged
    - Existing tests still pass (no regressions)
  </done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Add TestFlexibleIdeas test class covering IDEA-01 through IDEA-04</name>
  <files>
    tests/test_pipeline.py
  </files>
  <behavior>
    - Test IDEA-01: ideas_per_provider=1 → no selection LLM call → stage1_selections.jsonl has 4 valid records
    - Test IDEA-01: ideas_per_provider=1 → each selection record has all required SELECTION_SCHEMA fields
    - Test IDEA-02: ideas_per_provider=2 → selection LLM call IS made (mock generates a selection) → 4 selection records present
    - Test IDEA-03: ideas_per_provider=1 → stage1_feedback.jsonl exists and is non-empty
    - Test IDEA-04: ideas_per_provider=3 completes without error (count >= 1 always works)
    - Test IDEA-04: ideas_per_provider=1 completes full pipeline (Stage 2 + Stage 3 receive valid selections)
  </behavior>
  <action>
Append a new test class `TestFlexibleIdeas` to `tests/test_pipeline.py` after the existing `TestResume` class. Do NOT modify any existing tests.

```python
# ---------------------------------------------------------------------------
# Flexible Idea Count Tests
# ---------------------------------------------------------------------------


class TestFlexibleIdeas:
    """IDEA-01 through IDEA-04: flexible ideas_per_provider behavior."""

    def test_idea01_single_idea_no_selection_llm_call(self, tmp_path, monkeypatch):
        """IDEA-01: ideas_per_provider=1 skips the LLM selection call.

        We verify this by confirming the mock provider's generate() was NOT
        called with a 'select' context. Since MockProvider doesn't track calls,
        we use a counting wrapper and verify selection_task was not executed by
        checking that the single idea is auto-selected (reasoning contains
        'Auto-selected' or 'auto' keyword).
        """
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=1,
        )
        selections = _read_jsonl(run_dir / "stage1_selections.jsonl")
        assert len(selections) == 4, (
            f"Expected 4 selections (one per founder), got {len(selections)}"
        )
        # Verify each selection has required fields
        for sel in selections:
            assert "selected_idea_id" in sel
            assert "founder_provider" in sel
            assert "reasoning" in sel
            assert "refined_idea" in sel

    def test_idea01_single_idea_auto_selected_idea_id_matches_generated(
        self, tmp_path, monkeypatch
    ):
        """IDEA-01: the auto-selected idea_id matches the one generated in Stage 1."""
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=1,
        )
        ideas = _read_jsonl(run_dir / "stage1_ideas.jsonl")
        selections = _read_jsonl(run_dir / "stage1_selections.jsonl")

        # For each founder, the selected_idea_id must match the generated idea
        idea_ids_by_founder = {
            idea["proposer_provider"]: idea["idea_id"] for idea in ideas
        }
        for sel in selections:
            founder = sel["founder_provider"]
            expected_idea_id = idea_ids_by_founder.get(founder)
            assert sel["selected_idea_id"] == expected_idea_id, (
                f"Founder {founder}: selected_idea_id={sel['selected_idea_id']!r} "
                f"but generated idea_id={expected_idea_id!r}"
            )

    def test_idea03_feedback_runs_with_single_idea(self, tmp_path, monkeypatch):
        """IDEA-03: Stage 1 feedback step runs even when ideas_per_provider=1."""
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=1,
        )
        feedback_path = run_dir / "stage1_feedback.jsonl"
        assert feedback_path.exists(), "stage1_feedback.jsonl must be written even for count=1"
        feedback = _read_jsonl(feedback_path)
        assert len(feedback) > 0, (
            "stage1_feedback.jsonl must contain feedback items even when ideas_per_provider=1"
        )

    def test_idea02_two_ideas_runs_llm_selection(self, tmp_path, monkeypatch):
        """IDEA-02: ideas_per_provider=2 goes through the LLM selection path."""
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=2,
        )
        # 2 ideas per founder → normal LLM selection path must have fired
        ideas = _read_jsonl(run_dir / "stage1_ideas.jsonl")
        assert len(ideas) == 8, f"Expected 8 ideas (4 founders x 2), got {len(ideas)}"
        selections = _read_jsonl(run_dir / "stage1_selections.jsonl")
        assert len(selections) == 4, (
            f"Expected 4 selections, got {len(selections)}"
        )
        for sel in selections:
            assert "selected_idea_id" in sel
            assert "refined_idea" in sel

    def test_idea04_count_one_full_pipeline_completes(self, tmp_path, monkeypatch):
        """IDEA-04: ideas_per_provider=1 completes all 3 stages without errors."""
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=1,
        )
        assert run_dir.exists()
        # All stage outputs must be present
        assert (run_dir / "stage1_ideas.jsonl").exists()
        assert (run_dir / "stage1_feedback.jsonl").exists()
        assert (run_dir / "stage1_selections.jsonl").exists()
        assert (run_dir / "stage2_final_plans.jsonl").exists()
        assert (run_dir / "stage3_pitches.jsonl").exists()
        assert (run_dir / "portfolio_report.csv").exists()

    def test_idea04_count_three_still_works(self, tmp_path, monkeypatch):
        """IDEA-04: ideas_per_provider=3 (the original default region) still works."""
        monkeypatch.chdir(tmp_path)
        run_dir = run_pipeline(
            use_mock=True, concurrency=1, retry_max=1,
            max_iterations=1, ideas_per_provider=3,
        )
        ideas = _read_jsonl(run_dir / "stage1_ideas.jsonl")
        assert len(ideas) == 12, f"Expected 12 ideas (4 x 3), got {len(ideas)}"
        selections = _read_jsonl(run_dir / "stage1_selections.jsonl")
        assert len(selections) == 4
```

After writing, confirm the file ends with a newline and the class name is spelled correctly.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_pipeline.py::TestFlexibleIdeas -v 2>&1 | tail -30</automated>
  </verify>
  <done>
    - TestFlexibleIdeas class exists in tests/test_pipeline.py with 6 test methods
    - All 6 tests pass
    - Full test suite still passes: `pytest tests/ -v` shows 0 failures
    - IDEA-01 through IDEA-04 are covered by distinct test methods
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

```
cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/ -v 2>&1 | tail -30
```

Expected: All existing tests pass (including TestPipelineMock, TestRoleAssignment, TestDeliberation, TestParallelization, TestPreflight, TestResume) plus all 6 new TestFlexibleIdeas tests.

Spot-check: `python -m pytest tests/test_pipeline.py::TestFlexibleIdeas -v` shows 6 passed tests.
</verification>

<success_criteria>
1. `--ideas-per-provider 1` runs the full pipeline (all 3 stages) without calling the selection LLM — auto-selection synthetic result is written to stage1_selections.jsonl
2. Stage 1 feedback (stage1_feedback.jsonl) is non-empty even when ideas_per_provider=1
3. `--ideas-per-provider 2` produces 4 selection records via LLM call
4. select_prompt.txt no longer contains "pick the single best idea" — wording is count-neutral
5. `pytest tests/ -v` passes with 0 failures (63 existing + 6 new = 69 total)
</success_criteria>

<output>
After completion, create `.planning/phases/04-flexible-idea-count/04-01-SUMMARY.md` following the summary template.
</output>
