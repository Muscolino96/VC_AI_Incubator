---
phase: 09-live-cost-tracking
plan: 02
type: execute
wave: 2
depends_on: [09-01]
files_modified:
  - vc_agents/web/dashboard.html
  - tests/test_cost_tracker.py
autonomous: true
requirements: [COST-03, COST-01, COST-02, COST-04, COST-05]

must_haves:
  truths:
    - "Dashboard shows a live cost counter that updates each time a cost_update step_complete event arrives during a run"
    - "Cost counter is visible in the progress block alongside the stage indicators"
    - "Counter resets to $0.0000 when a new run starts"
    - "Test suite covers CostTracker pricing logic, running total accumulation, budget enforcement, and cost_report structure"
  artifacts:
    - path: "vc_agents/web/dashboard.html"
      provides: "Live cost counter in progress block, updated from step_complete cost_update events"
      contains: "costCounter"
    - path: "tests/test_cost_tracker.py"
      provides: "TestCostTracker: unit tests for CostTracker and BudgetExceeded"
      exports: ["TestCostTracker"]
  key_links:
    - from: "vc_agents/web/dashboard.html handleEvent()"
      to: "span#costCounter"
      via: "ev.type==='step_complete' && ev.step==='cost_update' → update textContent"
      pattern: "cost_update"
    - from: "tests/test_cost_tracker.py"
      to: "vc_agents/pipeline/cost_tracker.py"
      via: "CostTracker(providers, budget=N)"
      pattern: "CostTracker"
---

<objective>
Add the live cost counter to the dashboard and write the full test suite for CostTracker.

Purpose: Operators see real-time spend during pipeline execution (COST-03). Tests prove correctness of all cost logic (COST-01/02/04/05).
Output: Updated `dashboard.html`, new `tests/test_cost_tracker.py`.
</objective>

<execution_context>
@C:/Users/Vince/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Vince/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/09-live-cost-tracking/09-01-SUMMARY.md
@vc_agents/web/dashboard.html
@vc_agents/pipeline/cost_tracker.py
@tests/test_pipeline.py
</context>

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

Dashboard progress block HTML (lines 691-717 of dashboard.html):
```html
<div class="progress-block" id="progressBlock">
  <div class="section-label" style="margin-bottom:12px">Pipeline progress</div>
  <div class="progress-inner">
    <div class="progress-top">
      <div>
        <div class="progress-title">Running…</div>
        <div class="progress-sub" id="progressSub">Initialising</div>
      </div>
    </div>
    <div class="stage-track">...</div>
    <div class="pbar-track"><div class="pbar-fill" id="pbarFill" style="width:0%"></div></div>
    <div class="event-log" id="eventLog"></div>
  </div>
</div>
```

Dashboard handleEvent() function (line 1131):
```javascript
function handleEvent(ev) {
  document.getElementById('progressBlock').classList.add('visible');
  // ... existing handlers ...
  if (ev.type === 'step_complete') {
    const c = parseFloat(document.getElementById('pbarFill').style.width) || 0;
    document.getElementById('pbarFill').style.width = Math.min(c+1.5, 94) + '%';
  }
  if (ev.type === 'pipeline_complete') { ... }
  // Add cost counter update here
}
```

Dashboard startRun() (line 1186) — resets state when new run starts:
```javascript
function startRun() {
  const btn = document.getElementById('btnRun');
  btn.disabled = true;
  document.getElementById('runStatus').textContent = 'Starting…';
  ['pill1','pill2','pill3'].forEach(id => document.getElementById(id).classList.remove('active','done'));
  document.getElementById('pbarFill').style.width = '0%';
  document.getElementById('eventLog').innerHTML = '';
  // Add cost counter reset here
}
```

CSS variables already defined (from dashboard.html :root block):
```css
--gold: #d4a853;
--text-dim: #8a8c96;
--font-mono: 'JetBrains Mono', monospace;
--surface: #1a1b20;
--border: rgba(255,255,255,.07);
```

Event format emitted by run.py plan 01:
```python
PipelineEvent(
    type=EventType.STEP_COMPLETE, stage="stage1", step="cost_update",
    message="Running cost: $0.0012",
    data={"running_cost": 0.001234},
)
# Serialized as WebSocket JSON: { type: "step_complete", stage: "stage1", step: "cost_update", data: { running_cost: 0.001234 }, ... }
```

CostTracker interface from plan 01:
```python
class BudgetExceeded(RuntimeError):
    running_cost: float
    budget: float

class CostTracker:
    def __init__(self, providers: list[BaseProvider], budget: float | None = None): ...
    def record_step(self) -> float: ...  # returns cost increment
    def check_budget(self) -> None: ...  # raises BudgetExceeded if over limit
    @property
    def running_cost(self) -> float: ...
    def cost_report(self) -> dict: ...
    # Returns: {"total_cost_usd": float, "providers": {name: {"input_tokens": int, "output_tokens": int, "cost_usd": float}}, "budget_usd": float | None}
```

MockProvider usage — token counts stay at 0 throughout mock runs (no real API calls). Tests for budget enforcement must set budget=0.0 or manually inject token counts. The cleanest approach for unit tests is to use mock providers with patched usage values.
</interfaces>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Write TestCostTracker unit test suite</name>
  <files>tests/test_cost_tracker.py</files>
  <behavior>
    - test_pricing_lookup: CostTracker with a real provider whose model is in catalog computes non-zero cost
    - test_unknown_model_returns_zero_cost: model not in catalog → cost 0.0, no exception
    - test_running_total_accumulates: two record_step() calls, running_cost equals sum
    - test_budget_not_exceeded: running_cost below budget → check_budget() does not raise
    - test_budget_exceeded_raises: running_cost > budget → check_budget() raises BudgetExceeded
    - test_budget_exceeded_attributes: BudgetExceeded.running_cost and .budget are correct
    - test_budget_none_never_raises: budget=None → check_budget() never raises even with costs
    - test_cost_report_structure: cost_report() returns dict with total_cost_usd, providers (dict), budget_usd keys
    - test_cost_report_provider_keys: each provider appears in cost_report["providers"]
    - test_zero_delta_zero_cost: record_step() when no tokens consumed since last snapshot → 0.0 increment

    Use MockProvider instances (import from vc_agents.providers.mock). To simulate token consumption, manually set provider.usage.input_tokens and provider.usage.output_tokens before calling record_step(). MockProvider inherits BaseProvider so usage is a TokenUsage dataclass — just set the fields directly.

    For pricing tests: use a known model ID from models_catalog.yaml (e.g., "gpt-5-mini" with input=$0.25/M, output=$2.00/M). Set input_tokens=1_000_000 on the provider → expect cost ~0.25 for input only.
  </behavior>
  <action>
    Create `tests/test_cost_tracker.py`:

    ```python
    """Unit tests for CostTracker: pricing lookup, budget enforcement, cost_report."""

    import pytest
    from vc_agents.pipeline.cost_tracker import CostTracker, BudgetExceeded
    from vc_agents.providers.mock import MockProvider


    def _make_provider(name: str, model: str = "gpt-5-mini") -> MockProvider:
        """Return a MockProvider with .model attribute set."""
        p = MockProvider(name)
        p.model = model  # inject model string for pricing lookup
        return p


    class TestCostTracker:

        def test_pricing_lookup_known_model(self):
            """Known catalog model produces non-zero cost when tokens consumed."""
            p = _make_provider("openai", "gpt-5-mini")  # $0.25/$2.00 per 1M
            tracker = CostTracker([p])
            p.usage.input_tokens = 1_000_000
            p.usage.output_tokens = 0
            increment = tracker.record_step()
            assert increment == pytest.approx(0.25, abs=1e-6)
            assert tracker.running_cost == pytest.approx(0.25, abs=1e-6)

        def test_unknown_model_returns_zero(self):
            """Model not in catalog gives 0.0 cost without raising."""
            p = _make_provider("mystery", "nonexistent-model-xyz")
            tracker = CostTracker([p])
            p.usage.input_tokens = 1_000_000
            increment = tracker.record_step()
            assert increment == 0.0

        def test_running_total_accumulates_across_steps(self):
            """Two record_step() calls accumulate correctly."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p])
            p.usage.input_tokens = 1_000_000
            tracker.record_step()
            p.usage.input_tokens = 2_000_000
            tracker.record_step()
            # First: 1M * 0.25 = 0.25; Second: delta 1M * 0.25 = 0.25 → total 0.50
            assert tracker.running_cost == pytest.approx(0.50, abs=1e-6)

        def test_zero_delta_zero_increment(self):
            """No new tokens since last snapshot → 0.0 increment."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p])
            p.usage.input_tokens = 100
            tracker.record_step()
            increment = tracker.record_step()  # second call, no change
            assert increment == 0.0

        def test_budget_not_exceeded_no_raise(self):
            """running_cost < budget → check_budget() does not raise."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p], budget=10.0)
            p.usage.input_tokens = 1_000_000  # cost = $0.25
            tracker.record_step()
            tracker.check_budget()  # should not raise

        def test_budget_exceeded_raises(self):
            """running_cost > budget → check_budget() raises BudgetExceeded."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p], budget=0.10)
            p.usage.input_tokens = 1_000_000  # cost = $0.25 > $0.10
            tracker.record_step()
            with pytest.raises(BudgetExceeded):
                tracker.check_budget()

        def test_budget_exceeded_attributes(self):
            """BudgetExceeded carries .running_cost and .budget."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p], budget=0.10)
            p.usage.input_tokens = 1_000_000
            tracker.record_step()
            with pytest.raises(BudgetExceeded) as exc_info:
                tracker.check_budget()
            assert exc_info.value.running_cost == pytest.approx(0.25, abs=1e-4)
            assert exc_info.value.budget == pytest.approx(0.10, abs=1e-6)

        def test_budget_none_never_raises(self):
            """budget=None → check_budget() never raises regardless of cost."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p], budget=None)
            p.usage.input_tokens = 10_000_000  # huge cost
            tracker.record_step()
            tracker.check_budget()  # must not raise

        def test_cost_report_structure(self):
            """cost_report() returns required top-level keys."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p], budget=5.0)
            report = tracker.cost_report()
            assert "total_cost_usd" in report
            assert "providers" in report
            assert "budget_usd" in report
            assert report["budget_usd"] == pytest.approx(5.0)

        def test_cost_report_provider_keys(self):
            """Each provider appears in cost_report providers dict."""
            p1 = _make_provider("openai", "gpt-5-mini")
            p2 = _make_provider("anthropic", "claude-haiku-4-5")
            tracker = CostTracker([p1, p2])
            report = tracker.cost_report()
            assert "openai" in report["providers"]
            assert "anthropic" in report["providers"]

        def test_cost_report_total_matches_running_cost(self):
            """total_cost_usd in report equals running_cost property."""
            p = _make_provider("openai", "gpt-5-mini")
            tracker = CostTracker([p])
            p.usage.input_tokens = 500_000
            tracker.record_step()
            report = tracker.cost_report()
            assert report["total_cost_usd"] == pytest.approx(tracker.running_cost, abs=1e-8)

        def test_output_token_pricing(self):
            """Output tokens use the output pricing rate (not input rate)."""
            p = _make_provider("openai", "gpt-5-mini")  # output = $2.00/M
            tracker = CostTracker([p])
            p.usage.output_tokens = 1_000_000
            increment = tracker.record_step()
            assert increment == pytest.approx(2.00, abs=1e-6)
    ```

    Write this as a new file. Do NOT modify any existing test files.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -m pytest tests/test_cost_tracker.py -v 2>&1 | tail -20</automated>
  </verify>
  <done>All TestCostTracker tests pass (12 tests). Tests cover pricing, zero delta, budget enforcement, BudgetExceeded attributes, cost_report structure.</done>
</task>

<task type="auto">
  <name>Task 2: Add live cost counter to dashboard progress block</name>
  <files>vc_agents/web/dashboard.html</files>
  <action>
    Add a live cost counter to the dashboard's progress block. The counter lives in the `progress-top` div alongside the existing "Running… / Initialising" text. It updates when a `step_complete` event with `step === 'cost_update'` arrives.

    **CRITICAL: Do NOT simplify, flatten, or remove any existing CSS/JS. All changes are purely additive.**

    **1. Add HTML element for the cost counter in the progress block:**

    Find the `progress-top` div (around line 695):
    ```html
    <div class="progress-top">
      <div>
        <div class="progress-title">Running…</div>
        <div class="progress-sub" id="progressSub">Initialising</div>
      </div>
    </div>
    ```

    Change it to add the cost display on the right side of the flex row:
    ```html
    <div class="progress-top">
      <div>
        <div class="progress-title">Running…</div>
        <div class="progress-sub" id="progressSub">Initialising</div>
      </div>
      <div class="progress-cost" id="progressCost" style="display:none">
        <div class="cost-label">Spent</div>
        <div class="cost-value" id="costCounter">$0.0000</div>
      </div>
    </div>
    ```

    **2. Add CSS for the cost counter** (add in the existing CSS block, after the existing `.progress-inner`, `.progress-top` rules — find them and insert nearby. Do NOT replace any existing rules):

    ```css
    .progress-cost { text-align: right; }
    .cost-label { font-family: var(--font-mono); font-size: 10px; letter-spacing: 1.5px; text-transform: uppercase; color: var(--text-dim); margin-bottom: 3px; }
    .cost-value { font-family: var(--font-mono); font-size: 18px; font-weight: 500; color: var(--gold); }
    ```

    **3. Update handleEvent() in the JS section** — add a handler for `cost_update` step events. Add this line in `handleEvent()` after the existing `step_complete` block (around line 1145):

    ```javascript
    if (ev.type === 'step_complete' && ev.step === 'cost_update' && ev.data && ev.data.running_cost != null) {
      const costEl = document.getElementById('progressCost');
      const counterEl = document.getElementById('costCounter');
      if (costEl) costEl.style.display = '';
      if (counterEl) counterEl.textContent = '$' + ev.data.running_cost.toFixed(4);
    }
    ```

    **4. Reset cost counter in startRun()** — add after the existing resets in `startRun()`:
    ```javascript
    const costEl = document.getElementById('progressCost');
    const counterEl = document.getElementById('costCounter');
    if (costEl) costEl.style.display = 'none';
    if (counterEl) counterEl.textContent = '$0.0000';
    ```

    Find `startRun()` (around line 1186) and add these two lines after the `eventLog.innerHTML = ''` reset.

    Use the exact CSS variable names already in the file: `--font-mono`, `--text-dim`, `--gold`. Do not introduce new color variables.
  </action>
  <verify>
    <automated>cd C:/Users/Vince/Desktop/VC_AI_Incubator-main && python -c "
content = open('vc_agents/web/dashboard.html', encoding='utf-8').read()
assert 'costCounter' in content, 'costCounter element missing'
assert 'progressCost' in content, 'progressCost element missing'
assert 'cost_update' in content, 'cost_update handler missing'
assert 'cost-value' in content, 'cost-value CSS class missing'
print('Dashboard cost counter assertions passed')
"</automated>
  </verify>
  <done>
    - progress block contains #progressCost div and #costCounter span
    - handleEvent() updates costCounter when step=cost_update events arrive
    - startRun() resets counter to $0.0000 and hides the cost div
    - Existing dashboard CSS/JS is completely intact (no removals, no simplifications)
  </done>
</task>

</tasks>

<verification>
- `python -m pytest tests/ -v` — all tests pass including new TestCostTracker
- Dashboard smoke: `python -c "content = open('vc_agents/web/dashboard.html', encoding='utf-8').read(); assert 'costCounter' in content; assert 'cost_update' in content; print('OK')"`
- Full test count should be 80+ (existing) + 12 new CostTracker tests
</verification>

<success_criteria>
- TestCostTracker: 12 tests pass covering pricing, accumulation, budget, attributes, cost_report
- Dashboard has #costCounter element in progress block, styled with existing CSS variables
- handleEvent() updates counter on cost_update step events
- startRun() resets counter
- No existing dashboard CSS/JS removed or simplified
- Total pytest test count >= 92
</success_criteria>

<output>
After completion, create `.planning/phases/09-live-cost-tracking/09-02-SUMMARY.md`
</output>
